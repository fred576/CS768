{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/utkarsh/miniconda3/envs/aiml/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from model import GNNGlobal, GNNTopK, GNNSAG, GNNDiffPool, GNNGlobalAttention, GNNSet2Set, GNNDMoN, GNNECPool, GNNMinCut\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.data import DataLoader\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import pandas as pd\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index, data.batch)  # Forward pass\n",
    "        loss = F.cross_entropy(out, data.y)   # Compute cross-entropy loss between predictions and true labels\n",
    "        loss.backward()     # Backpropagation\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "    return total_loss / len(loader.dataset)  # Return average loss per graph             # Return average loss per graph\n",
    "\n",
    "\"\"\"Evaluate model accuracy on the given DataLoader.\"\"\"\n",
    "@torch.no_grad()   # Disables autograd tracking\n",
    "def test(model, loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        pred = model(data.x, data.edge_index, data.batch).argmax(dim=1)   # Predict class by selecting highest logit\n",
    "        correct += (pred == data.y).sum().item()\n",
    "    \n",
    "    return correct / len(loader.dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for name in ['MUTAG','PROTEINS','ENZYMES']:\n",
    "    \n",
    "    dataset = TUDataset(root='data', name=name)    # Load the graph classification dataset into a PyTorch Geometric TUDataset object\n",
    "    dataset = dataset.shuffle()    # Shuffle the dataset to avoid any potential ordering bias when splitting\n",
    "\n",
    "    # Compute the index at which to split the dataset:\n",
    "    # 80% for training, 20% for testing\n",
    "    split = int(0.8 * len(dataset))\n",
    "    train_ds, test_ds = dataset[:split], dataset[split:]  # Split the shuffled dataset into train and test datasets\n",
    "\n",
    "    # Wrap the training subset in a DataLoader to yield batches of 32 graphs\n",
    "    train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "\n",
    "    # Wrap the test subset in a DataLoader to yield batches of 32 graphs\n",
    "    # during evaluation; shuffling is disabled to preserve consistency\n",
    "    test_loader  = DataLoader(test_ds,  batch_size=32)\n",
    "    \n",
    "    # -----------------------------------------------------------------------------\n",
    "    # 3. Hyperparameters & Device Configuration\n",
    "    # -----------------------------------------------------------------------------\n",
    "\n",
    "    in_feats    = dataset.num_features   # Number of node features per graph\n",
    "    hidden_dim  = 64                     # Hidden dimension size for internal GCN layers\n",
    "    num_classes = dataset.num_classes    # Number of target classes for graph-level classification\n",
    "    device      = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # Compute device\n",
    "    print(f\"Using device: {device}\")\n",
    "    print(f\"Dataset: {name}, Number of classes: {num_classes}, Number of features: {in_feats}\")\n",
    "   # Return accuracy over all graphs\n",
    "\n",
    "    # -----------------------------------------------------------------------------\n",
    "    # 5. Model Instantiation & Training Loop\n",
    "    # -----------------------------------------------------------------------------\n",
    "\n",
    "    # Define different GNN variants to compare\n",
    "    variants = {\n",
    "        # 'no-pool':     GNNGlobal(in_feats, hidden_dim, num_classes, pool='none'),\n",
    "        # 'global-max':  GNNGlobal(in_feats, hidden_dim, num_classes, pool='max'),\n",
    "        'dmon':          GNNDMoN(in_feats, hidden_dim, num_classes, k=10, dropout=0.2),\n",
    "        # 'ecpool':        GNNECPool(in_feats, hidden_dim, num_classes, ratio=0.5),\n",
    "        # 'mincut':        GNNMinCut(in_feats, hidden_dim, num_classes, k=10, temp = 1.0),\n",
    "        # 'global-att':  GNNGlobalAttention(in_feats, hidden_dim, num_classes),\n",
    "        # 'set2set':     GNNSet2Set(in_feats, hidden_dim, num_classes),\n",
    "        # 'global-mean': GNNGlobal(in_feats, hidden_dim, num_classes, pool='mean'),\n",
    "        # 'global-sum':  GNNGlobal(in_feats, hidden_dim, num_classes, pool='sum'),\n",
    "        # 'topk':        GNNTopK(in_feats, hidden_dim, num_classes),\n",
    "        # 'sag':         GNNSAG(in_feats, hidden_dim, num_classes, ratio=0.5),\n",
    "        # 'diff':        GNNDiffPool(in_feats, assign_dim=hidden_dim, k=30, num_classes=num_classes)\n",
    "    }\n",
    "\n",
    "    print(f\"For {name} dataset:\")\n",
    "    for pool, model in variants.items():\n",
    "        model = model.to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "        start_time = time.time()\n",
    "        for epoch in range(1, 101):\n",
    "            loss = train(model, train_loader, optimizer, device)\n",
    "        total_time = time.time() - start_time\n",
    "        time_per_epoch = total_time / 100\n",
    "\n",
    "        acc = test(model, test_loader, device)\n",
    "\n",
    "        if device.type == 'cuda':\n",
    "            peak_mem = torch.cuda.max_memory_allocated(device) / (1024 ** 2)  # Convert to MB\n",
    "        else:\n",
    "            peak_mem = None  # Could use psutil here for CPU if needed\n",
    "\n",
    "        print(f\"{pool} test acc: {acc:.4f}, time/epoch: {time_per_epoch:.4f}s, memory: {peak_mem:.2f} MB\")\n",
    "        torch.save(model.state_dict(), f\"trained_models/{name}_{pool}_model.pth\")\n",
    "        results.append({\n",
    "            'pool': pool,\n",
    "            'dataset': name,\n",
    "            's/epoch': round(time_per_epoch, 4),\n",
    "            'memory_MB': round(peak_mem, 2) if peak_mem is not None else 'N/A',\n",
    "            'accuracy': round(acc, 4)\n",
    "        })\n",
    "\n",
    "        # Cleanup to avoid CUDA OOM across variants\n",
    "        del model\n",
    "        gc.collect()\n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "# Convert to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(f'gnn_pooling_benchmark.csv', index=False)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/utkarsh/miniconda3/envs/aiml/lib/python3.7/site-packages/torch_geometric/deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Dataset: MUTAG, Number of classes: 2, Number of features: 7\n",
      "For MUTAG dataset with k=1:\n",
      "k=1, test acc: 0.868421052631579, time/epoch: 0.0231s\n",
      "For MUTAG dataset with k=2:\n",
      "k=2, test acc: 0.868421052631579, time/epoch: 0.0334s\n",
      "For MUTAG dataset with k=3:\n",
      "k=3, test acc: 0.868421052631579, time/epoch: 0.0275s\n",
      "For MUTAG dataset with k=4:\n",
      "k=4, test acc: 0.868421052631579, time/epoch: 0.0300s\n",
      "For MUTAG dataset with k=5:\n",
      "k=5, test acc: 0.868421052631579, time/epoch: 0.0320s\n",
      "For MUTAG dataset with k=6:\n",
      "k=6, test acc: 0.868421052631579, time/epoch: 0.0298s\n",
      "For MUTAG dataset with k=7:\n",
      "k=7, test acc: 0.868421052631579, time/epoch: 0.0189s\n",
      "For MUTAG dataset with k=8:\n",
      "k=8, test acc: 0.868421052631579, time/epoch: 0.0334s\n",
      "For MUTAG dataset with k=9:\n",
      "k=9, test acc: 0.8421052631578947, time/epoch: 0.0362s\n",
      "For MUTAG dataset with k=10:\n",
      "k=10, test acc: 0.8157894736842105, time/epoch: 0.0363s\n",
      "For MUTAG dataset with k=11:\n",
      "k=11, test acc: 0.868421052631579, time/epoch: 0.0190s\n",
      "For MUTAG dataset with k=12:\n",
      "k=12, test acc: 0.8157894736842105, time/epoch: 0.0188s\n",
      "For MUTAG dataset with k=13:\n",
      "k=13, test acc: 0.868421052631579, time/epoch: 0.0191s\n",
      "For MUTAG dataset with k=14:\n",
      "k=14, test acc: 0.8421052631578947, time/epoch: 0.0206s\n",
      "For MUTAG dataset with k=15:\n",
      "k=15, test acc: 0.868421052631579, time/epoch: 0.0224s\n",
      "For MUTAG dataset with k=16:\n",
      "k=16, test acc: 0.868421052631579, time/epoch: 0.0225s\n",
      "For MUTAG dataset with k=17:\n",
      "k=17, test acc: 0.868421052631579, time/epoch: 0.0287s\n",
      "For MUTAG dataset with k=18:\n",
      "k=18, test acc: 0.8421052631578947, time/epoch: 0.0390s\n",
      "For MUTAG dataset with k=19:\n",
      "k=19, test acc: 0.868421052631579, time/epoch: 0.0239s\n",
      "For MUTAG dataset with k=20:\n",
      "k=20, test acc: 0.868421052631579, time/epoch: 0.0220s\n",
      "Using device: cpu\n",
      "Dataset: PROTEINS, Number of classes: 2, Number of features: 3\n",
      "For PROTEINS dataset with k=1:\n",
      "k=1, test acc: 0.7488789237668162, time/epoch: 0.2926s\n",
      "For PROTEINS dataset with k=2:\n",
      "k=2, test acc: 0.7488789237668162, time/epoch: 0.2439s\n",
      "For PROTEINS dataset with k=3:\n",
      "k=3, test acc: 0.7443946188340808, time/epoch: 0.2684s\n",
      "For PROTEINS dataset with k=4:\n",
      "k=4, test acc: 0.7533632286995515, time/epoch: 0.2765s\n",
      "For PROTEINS dataset with k=5:\n",
      "k=5, test acc: 0.7443946188340808, time/epoch: 0.2642s\n",
      "For PROTEINS dataset with k=6:\n",
      "k=6, test acc: 0.7399103139013453, time/epoch: 0.2592s\n",
      "For PROTEINS dataset with k=7:\n",
      "k=7, test acc: 0.757847533632287, time/epoch: 0.2044s\n",
      "For PROTEINS dataset with k=8:\n",
      "k=8, test acc: 0.7488789237668162, time/epoch: 0.1980s\n",
      "For PROTEINS dataset with k=9:\n",
      "k=9, test acc: 0.7443946188340808, time/epoch: 0.2030s\n",
      "For PROTEINS dataset with k=10:\n",
      "k=10, test acc: 0.7399103139013453, time/epoch: 0.2056s\n",
      "For PROTEINS dataset with k=11:\n",
      "k=11, test acc: 0.7443946188340808, time/epoch: 0.2072s\n",
      "For PROTEINS dataset with k=12:\n",
      "k=12, test acc: 0.757847533632287, time/epoch: 0.2093s\n",
      "For PROTEINS dataset with k=13:\n",
      "k=13, test acc: 0.7443946188340808, time/epoch: 0.2290s\n",
      "For PROTEINS dataset with k=14:\n",
      "k=14, test acc: 0.7533632286995515, time/epoch: 0.3311s\n",
      "For PROTEINS dataset with k=15:\n",
      "k=15, test acc: 0.7443946188340808, time/epoch: 0.3175s\n",
      "For PROTEINS dataset with k=16:\n",
      "k=16, test acc: 0.7533632286995515, time/epoch: 0.2102s\n",
      "For PROTEINS dataset with k=17:\n",
      "k=17, test acc: 0.7488789237668162, time/epoch: 0.2196s\n",
      "For PROTEINS dataset with k=18:\n",
      "k=18, test acc: 0.7623318385650224, time/epoch: 0.2199s\n",
      "For PROTEINS dataset with k=19:\n",
      "k=19, test acc: 0.757847533632287, time/epoch: 0.2226s\n",
      "For PROTEINS dataset with k=20:\n",
      "k=20, test acc: 0.7309417040358744, time/epoch: 0.3323s\n",
      "Using device: cpu\n",
      "Dataset: ENZYMES, Number of classes: 6, Number of features: 3\n",
      "For ENZYMES dataset with k=1:\n",
      "k=1, test acc: 0.2833333333333333, time/epoch: 0.0815s\n",
      "For ENZYMES dataset with k=2:\n",
      "k=2, test acc: 0.275, time/epoch: 0.1130s\n",
      "For ENZYMES dataset with k=3:\n",
      "k=3, test acc: 0.31666666666666665, time/epoch: 0.1094s\n",
      "For ENZYMES dataset with k=4:\n",
      "k=4, test acc: 0.3333333333333333, time/epoch: 0.1126s\n",
      "For ENZYMES dataset with k=5:\n",
      "k=5, test acc: 0.2916666666666667, time/epoch: 0.1124s\n",
      "For ENZYMES dataset with k=6:\n",
      "k=6, test acc: 0.31666666666666665, time/epoch: 0.1277s\n",
      "For ENZYMES dataset with k=7:\n",
      "k=7, test acc: 0.275, time/epoch: 0.1492s\n",
      "For ENZYMES dataset with k=8:\n",
      "k=8, test acc: 0.2916666666666667, time/epoch: 0.1422s\n",
      "For ENZYMES dataset with k=9:\n",
      "k=9, test acc: 0.30833333333333335, time/epoch: 0.1449s\n",
      "For ENZYMES dataset with k=10:\n",
      "k=10, test acc: 0.26666666666666666, time/epoch: 0.0948s\n",
      "For ENZYMES dataset with k=11:\n",
      "k=11, test acc: 0.38333333333333336, time/epoch: 0.0917s\n",
      "For ENZYMES dataset with k=12:\n",
      "k=12, test acc: 0.30833333333333335, time/epoch: 0.0845s\n",
      "For ENZYMES dataset with k=13:\n",
      "k=13, test acc: 0.325, time/epoch: 0.0858s\n",
      "For ENZYMES dataset with k=14:\n",
      "k=14, test acc: 0.30833333333333335, time/epoch: 0.0873s\n",
      "For ENZYMES dataset with k=15:\n",
      "k=15, test acc: 0.31666666666666665, time/epoch: 0.1486s\n",
      "For ENZYMES dataset with k=16:\n",
      "k=16, test acc: 0.35833333333333334, time/epoch: 0.0863s\n",
      "For ENZYMES dataset with k=17:\n",
      "k=17, test acc: 0.325, time/epoch: 0.0890s\n",
      "For ENZYMES dataset with k=18:\n",
      "k=18, test acc: 0.31666666666666665, time/epoch: 0.0899s\n",
      "For ENZYMES dataset with k=19:\n",
      "k=19, test acc: 0.31666666666666665, time/epoch: 0.0907s\n",
      "For ENZYMES dataset with k=20:\n",
      "k=20, test acc: 0.31666666666666665, time/epoch: 0.0914s\n",
      "     k   dataset  s/epoch memory_MB  accuracy\n",
      "0    1     MUTAG   0.0231       N/A    0.8684\n",
      "1    2     MUTAG   0.0334       N/A    0.8684\n",
      "2    3     MUTAG   0.0275       N/A    0.8684\n",
      "3    4     MUTAG   0.0300       N/A    0.8684\n",
      "4    5     MUTAG   0.0320       N/A    0.8684\n",
      "5    6     MUTAG   0.0298       N/A    0.8684\n",
      "6    7     MUTAG   0.0189       N/A    0.8684\n",
      "7    8     MUTAG   0.0334       N/A    0.8684\n",
      "8    9     MUTAG   0.0362       N/A    0.8421\n",
      "9   10     MUTAG   0.0363       N/A    0.8158\n",
      "10  11     MUTAG   0.0190       N/A    0.8684\n",
      "11  12     MUTAG   0.0188       N/A    0.8158\n",
      "12  13     MUTAG   0.0191       N/A    0.8684\n",
      "13  14     MUTAG   0.0206       N/A    0.8421\n",
      "14  15     MUTAG   0.0224       N/A    0.8684\n",
      "15  16     MUTAG   0.0225       N/A    0.8684\n",
      "16  17     MUTAG   0.0287       N/A    0.8684\n",
      "17  18     MUTAG   0.0390       N/A    0.8421\n",
      "18  19     MUTAG   0.0239       N/A    0.8684\n",
      "19  20     MUTAG   0.0220       N/A    0.8684\n",
      "20   1  PROTEINS   0.2926       N/A    0.7489\n",
      "21   2  PROTEINS   0.2439       N/A    0.7489\n",
      "22   3  PROTEINS   0.2684       N/A    0.7444\n",
      "23   4  PROTEINS   0.2765       N/A    0.7534\n",
      "24   5  PROTEINS   0.2642       N/A    0.7444\n",
      "25   6  PROTEINS   0.2592       N/A    0.7399\n",
      "26   7  PROTEINS   0.2044       N/A    0.7578\n",
      "27   8  PROTEINS   0.1980       N/A    0.7489\n",
      "28   9  PROTEINS   0.2030       N/A    0.7444\n",
      "29  10  PROTEINS   0.2056       N/A    0.7399\n",
      "30  11  PROTEINS   0.2072       N/A    0.7444\n",
      "31  12  PROTEINS   0.2093       N/A    0.7578\n",
      "32  13  PROTEINS   0.2290       N/A    0.7444\n",
      "33  14  PROTEINS   0.3311       N/A    0.7534\n",
      "34  15  PROTEINS   0.3175       N/A    0.7444\n",
      "35  16  PROTEINS   0.2102       N/A    0.7534\n",
      "36  17  PROTEINS   0.2196       N/A    0.7489\n",
      "37  18  PROTEINS   0.2199       N/A    0.7623\n",
      "38  19  PROTEINS   0.2226       N/A    0.7578\n",
      "39  20  PROTEINS   0.3323       N/A    0.7309\n",
      "40   1   ENZYMES   0.0815       N/A    0.2833\n",
      "41   2   ENZYMES   0.1130       N/A    0.2750\n",
      "42   3   ENZYMES   0.1094       N/A    0.3167\n",
      "43   4   ENZYMES   0.1126       N/A    0.3333\n",
      "44   5   ENZYMES   0.1124       N/A    0.2917\n",
      "45   6   ENZYMES   0.1277       N/A    0.3167\n",
      "46   7   ENZYMES   0.1492       N/A    0.2750\n",
      "47   8   ENZYMES   0.1422       N/A    0.2917\n",
      "48   9   ENZYMES   0.1449       N/A    0.3083\n",
      "49  10   ENZYMES   0.0948       N/A    0.2667\n",
      "50  11   ENZYMES   0.0917       N/A    0.3833\n",
      "51  12   ENZYMES   0.0845       N/A    0.3083\n",
      "52  13   ENZYMES   0.0858       N/A    0.3250\n",
      "53  14   ENZYMES   0.0873       N/A    0.3083\n",
      "54  15   ENZYMES   0.1486       N/A    0.3167\n",
      "55  16   ENZYMES   0.0863       N/A    0.3583\n",
      "56  17   ENZYMES   0.0890       N/A    0.3250\n",
      "57  18   ENZYMES   0.0899       N/A    0.3167\n",
      "58  19   ENZYMES   0.0907       N/A    0.3167\n",
      "59  20   ENZYMES   0.0914       N/A    0.3167\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for name in ['MUTAG', 'PROTEINS', 'ENZYMES']:\n",
    "    dataset = TUDataset(root='data', name=name)  # Load the graph classification dataset\n",
    "    dataset = dataset.shuffle()  # Shuffle the dataset\n",
    "\n",
    "    # Split the dataset into training and testing subsets\n",
    "    split = int(0.8 * len(dataset))\n",
    "    train_ds, test_ds = dataset[:split], dataset[split:]\n",
    "    train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(test_ds, batch_size=32)\n",
    "\n",
    "    in_feats = dataset.num_features\n",
    "    hidden_dim = 64\n",
    "    num_classes = dataset.num_classes\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    print(f\"Dataset: {name}, Number of classes: {num_classes}, Number of features: {in_feats}\")\n",
    "\n",
    "    # Grid search over k values\n",
    "    for k in list(range(1,21)):  # Define the range of k values to test\n",
    "        print(f\"For {name} dataset with k={k}:\")\n",
    "        model = GNNDMoN(in_feats, hidden_dim, num_classes, k=k, dropout=0.2).to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "        start_time = time.time()\n",
    "        for epoch in range(1, 101):\n",
    "            loss = train(model, train_loader, optimizer, device)\n",
    "        total_time = time.time() - start_time\n",
    "        time_per_epoch = total_time / 100\n",
    "\n",
    "        acc = test(model, test_loader, device)\n",
    "\n",
    "        if device.type == 'cuda':\n",
    "            peak_mem = torch.cuda.max_memory_allocated(device) / (1024 ** 2)  # Convert to MB\n",
    "        else:\n",
    "            peak_mem = None\n",
    "\n",
    "        print(f\"k={k}, test acc: {acc}, time/epoch: {time_per_epoch:.4f}s\")\n",
    "        # torch.save(model.state_dict(), f\"trained_models/{name}_dmon_k{k}_model.pth\")\n",
    "        results.append({\n",
    "            'k': k,\n",
    "            'dataset': name,\n",
    "            's/epoch': round(time_per_epoch, 4),\n",
    "            'memory_MB': round(peak_mem, 2) if peak_mem is not None else 'N/A',\n",
    "            'accuracy': round(acc, 4)\n",
    "        })\n",
    "\n",
    "        # Cleanup to avoid CUDA OOM across variants\n",
    "        del model\n",
    "        gc.collect()\n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "# Convert to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "# results_df.to_csv(f'gnn_pooling_benchmark_k_grid_search.csv', index=False)\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://www.chrsmrrs.com/graphkerneldatasets/COLLAB.zip\n",
      "Extracting data/COLLAB/COLLAB.zip\n",
      "Processing...\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for name in ['COLLAB']:\n",
    "    dataset = TUDataset(root='data', name=name)  # Load the graph classification dataset\n",
    "    dataset = dataset.shuffle()  # Shuffle the dataset\n",
    "\n",
    "    # Split the dataset into training and testing subsets\n",
    "    split = int(0.8 * len(dataset))\n",
    "    train_ds, test_ds = dataset[:split], dataset[split:]\n",
    "    train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(test_ds, batch_size=32)\n",
    "\n",
    "    in_feats = dataset.num_features\n",
    "    hidden_dim = 64\n",
    "    num_classes = dataset.num_classes\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    print(f\"Dataset: {name}, Number of classes: {num_classes}, Number of features: {in_feats}\")\n",
    "\n",
    "    # Grid search over k values\n",
    "    for k in list(range(5,61,5)):  # Define the range of k values to test\n",
    "        print(f\"For {name} dataset with k={k}:\")\n",
    "        model = GNNDMoN(in_feats, hidden_dim, num_classes, k=k, dropout=0.2).to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "        start_time = time.time()\n",
    "        for epoch in range(1, 101):\n",
    "            loss = train(model, train_loader, optimizer, device)\n",
    "        total_time = time.time() - start_time\n",
    "        time_per_epoch = total_time / 100\n",
    "\n",
    "        acc = test(model, test_loader, device)\n",
    "\n",
    "        if device.type == 'cuda':\n",
    "            peak_mem = torch.cuda.max_memory_allocated(device) / (1024 ** 2)  # Convert to MB\n",
    "        else:\n",
    "            peak_mem = None\n",
    "\n",
    "        print(f\"k={k}, test acc: {acc}, time/epoch: {time_per_epoch:.4f}s\")\n",
    "        # torch.save(model.state_dict(), f\"trained_models/{name}_dmon_k{k}_model.pth\")\n",
    "        results.append({\n",
    "            'k': k,\n",
    "            'dataset': name,\n",
    "            's/epoch': round(time_per_epoch, 4),\n",
    "            'memory_MB': round(peak_mem, 2) if peak_mem is not None else 'N/A',\n",
    "            'accuracy': round(acc, 4)\n",
    "        })\n",
    "\n",
    "        # Cleanup to avoid CUDA OOM across variants\n",
    "        del model\n",
    "        gc.collect()\n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "# Convert to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "# results_df.to_csv(f'gnn_pooling_benchmark_k_grid_search.csv', index=False)\n",
    "print(results_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
