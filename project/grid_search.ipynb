{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import GNNGlobal, GNNTopK, GNNSAG, GNNDiffPool, GNNGlobalAttention, GNNSet2Set, GNNDMoN, GNNECPool, GNNMinCut\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.data import DataLoader\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import pandas as pd\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index, data.batch)  # Forward pass\n",
    "        loss = F.cross_entropy(out, data.y)   # Compute cross-entropy loss between predictions and true labels\n",
    "        loss.backward()     # Backpropagation\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "    return total_loss / len(loader.dataset)  # Return average loss per graph             # Return average loss per graph\n",
    "\n",
    "\"\"\"Evaluate model accuracy on the given DataLoader.\"\"\"\n",
    "@torch.no_grad()   # Disables autograd tracking\n",
    "def test(model, loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        pred = model(data.x, data.edge_index, data.batch).argmax(dim=1)   # Predict class by selecting highest logit\n",
    "        correct += (pred == data.y).sum().item()\n",
    "    \n",
    "    return correct / len(loader.dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Dataset: MUTAG, Number of classes: 2, Number of features: 7\n",
      "For MUTAG dataset with ratio =0.1:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/utkarsh/miniconda3/envs/aiml/lib/python3.7/site-packages/torch_geometric/deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=0.1, test acc: 0.8157894736842105, time/epoch: 0.0591s\n",
      "For MUTAG dataset with ratio =0.2:\n",
      "k=0.2, test acc: 0.8157894736842105, time/epoch: 0.0523s\n",
      "For MUTAG dataset with ratio =0.3:\n",
      "k=0.3, test acc: 0.8421052631578947, time/epoch: 0.0512s\n",
      "For MUTAG dataset with ratio =0.4:\n",
      "k=0.4, test acc: 0.8421052631578947, time/epoch: 0.0569s\n",
      "For MUTAG dataset with ratio =0.5:\n",
      "k=0.5, test acc: 0.868421052631579, time/epoch: 0.0485s\n",
      "For MUTAG dataset with ratio =0.6:\n",
      "k=0.6, test acc: 0.7894736842105263, time/epoch: 0.0524s\n",
      "For MUTAG dataset with ratio =0.7:\n",
      "k=0.7, test acc: 0.8421052631578947, time/epoch: 0.0514s\n",
      "For MUTAG dataset with ratio =0.8:\n",
      "k=0.8, test acc: 0.868421052631579, time/epoch: 0.0510s\n",
      "For MUTAG dataset with ratio =0.9:\n",
      "k=0.9, test acc: 0.8157894736842105, time/epoch: 0.0599s\n",
      "Using device: cpu\n",
      "Dataset: PROTEINS, Number of classes: 2, Number of features: 3\n",
      "For PROTEINS dataset with ratio =0.1:\n",
      "k=0.1, test acc: 0.7354260089686099, time/epoch: 0.7300s\n",
      "For PROTEINS dataset with ratio =0.2:\n",
      "k=0.2, test acc: 0.7085201793721974, time/epoch: 0.7104s\n",
      "For PROTEINS dataset with ratio =0.3:\n",
      "k=0.3, test acc: 0.7130044843049327, time/epoch: 0.6958s\n",
      "For PROTEINS dataset with ratio =0.4:\n",
      "k=0.4, test acc: 0.695067264573991, time/epoch: 0.6920s\n",
      "For PROTEINS dataset with ratio =0.5:\n",
      "k=0.5, test acc: 0.7174887892376681, time/epoch: 0.6849s\n",
      "For PROTEINS dataset with ratio =0.6:\n",
      "k=0.6, test acc: 0.7309417040358744, time/epoch: 0.6926s\n",
      "For PROTEINS dataset with ratio =0.7:\n",
      "k=0.7, test acc: 0.6905829596412556, time/epoch: 0.7001s\n",
      "For PROTEINS dataset with ratio =0.8:\n",
      "k=0.8, test acc: 0.7040358744394619, time/epoch: 0.6934s\n",
      "For PROTEINS dataset with ratio =0.9:\n",
      "k=0.9, test acc: 0.695067264573991, time/epoch: 0.6933s\n",
      "Using device: cpu\n",
      "Dataset: ENZYMES, Number of classes: 6, Number of features: 3\n",
      "For ENZYMES dataset with ratio =0.1:\n",
      "k=0.1, test acc: 0.2916666666666667, time/epoch: 0.3335s\n",
      "For ENZYMES dataset with ratio =0.2:\n",
      "k=0.2, test acc: 0.26666666666666666, time/epoch: 0.3359s\n",
      "For ENZYMES dataset with ratio =0.3:\n",
      "k=0.3, test acc: 0.25833333333333336, time/epoch: 0.3313s\n",
      "For ENZYMES dataset with ratio =0.4:\n",
      "k=0.4, test acc: 0.25, time/epoch: 0.3326s\n",
      "For ENZYMES dataset with ratio =0.5:\n",
      "k=0.5, test acc: 0.2833333333333333, time/epoch: 0.3297s\n",
      "For ENZYMES dataset with ratio =0.6:\n",
      "k=0.6, test acc: 0.24166666666666667, time/epoch: 0.3281s\n",
      "For ENZYMES dataset with ratio =0.7:\n",
      "k=0.7, test acc: 0.20833333333333334, time/epoch: 0.3332s\n",
      "For ENZYMES dataset with ratio =0.8:\n",
      "k=0.8, test acc: 0.26666666666666666, time/epoch: 0.3352s\n",
      "For ENZYMES dataset with ratio =0.9:\n",
      "k=0.9, test acc: 0.25833333333333336, time/epoch: 0.3376s\n",
      "      k   dataset  s/epoch memory_MB  accuracy\n",
      "0   0.1     MUTAG   0.0591       N/A    0.8158\n",
      "1   0.2     MUTAG   0.0523       N/A    0.8158\n",
      "2   0.3     MUTAG   0.0512       N/A    0.8421\n",
      "3   0.4     MUTAG   0.0569       N/A    0.8421\n",
      "4   0.5     MUTAG   0.0485       N/A    0.8684\n",
      "5   0.6     MUTAG   0.0524       N/A    0.7895\n",
      "6   0.7     MUTAG   0.0514       N/A    0.8421\n",
      "7   0.8     MUTAG   0.0510       N/A    0.8684\n",
      "8   0.9     MUTAG   0.0599       N/A    0.8158\n",
      "9   0.1  PROTEINS   0.7300       N/A    0.7354\n",
      "10  0.2  PROTEINS   0.7104       N/A    0.7085\n",
      "11  0.3  PROTEINS   0.6958       N/A    0.7130\n",
      "12  0.4  PROTEINS   0.6920       N/A    0.6951\n",
      "13  0.5  PROTEINS   0.6849       N/A    0.7175\n",
      "14  0.6  PROTEINS   0.6926       N/A    0.7309\n",
      "15  0.7  PROTEINS   0.7001       N/A    0.6906\n",
      "16  0.8  PROTEINS   0.6934       N/A    0.7040\n",
      "17  0.9  PROTEINS   0.6933       N/A    0.6951\n",
      "18  0.1   ENZYMES   0.3335       N/A    0.2917\n",
      "19  0.2   ENZYMES   0.3359       N/A    0.2667\n",
      "20  0.3   ENZYMES   0.3313       N/A    0.2583\n",
      "21  0.4   ENZYMES   0.3326       N/A    0.2500\n",
      "22  0.5   ENZYMES   0.3297       N/A    0.2833\n",
      "23  0.6   ENZYMES   0.3281       N/A    0.2417\n",
      "24  0.7   ENZYMES   0.3332       N/A    0.2083\n",
      "25  0.8   ENZYMES   0.3352       N/A    0.2667\n",
      "26  0.9   ENZYMES   0.3376       N/A    0.2583\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for name in ['MUTAG', 'PROTEINS', 'ENZYMES']:\n",
    "    dataset = TUDataset(root='data', name=name)  # Load the graph classification dataset\n",
    "    dataset = dataset.shuffle()  # Shuffle the dataset\n",
    "\n",
    "    # Split the dataset into training and testing subsets\n",
    "    split = int(0.8 * len(dataset))\n",
    "    train_ds, test_ds = dataset[:split], dataset[split:]\n",
    "    train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(test_ds, batch_size=32)\n",
    "\n",
    "    in_feats = dataset.num_features\n",
    "    hidden_dim = 64\n",
    "    num_classes = dataset.num_classes\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    print(f\"Dataset: {name}, Number of classes: {num_classes}, Number of features: {in_feats}\")\n",
    "\n",
    "    # Grid search over k values\n",
    "    for r in [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]:  # Define the range of k values to test\n",
    "        print(f\"For {name} dataset with ratio ={r}:\")\n",
    "        model = GNNECPool(in_feats, hidden_dim, num_classes, ratio = r).to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "        start_time = time.time()\n",
    "        for epoch in range(1, 101):\n",
    "            loss = train(model, train_loader, optimizer, device)\n",
    "        total_time = time.time() - start_time\n",
    "        time_per_epoch = total_time / 100\n",
    "\n",
    "        acc = test(model, test_loader, device)\n",
    "\n",
    "        if device.type == 'cuda':\n",
    "            peak_mem = torch.cuda.max_memory_allocated(device) / (1024 ** 2)  # Convert to MB\n",
    "        else:\n",
    "            peak_mem = None\n",
    "\n",
    "        print(f\"k={r}, test acc: {acc}, time/epoch: {time_per_epoch:.4f}s\")\n",
    "        # torch.save(model.state_dict(), f\"trained_models/{name}_dmon_k{k}_model.pth\")\n",
    "        results.append({\n",
    "            'k': r,\n",
    "            'dataset': name,\n",
    "            's/epoch': round(time_per_epoch, 4),\n",
    "            'memory_MB': round(peak_mem, 2) if peak_mem is not None else 'N/A',\n",
    "            'accuracy': round(acc, 4)\n",
    "        })\n",
    "\n",
    "        # Cleanup to avoid CUDA OOM across variants\n",
    "        del model\n",
    "        gc.collect()\n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "# Convert to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "# results_df.to_csv(f'gnn_pooling_benchmark_k_grid_search.csv', index=False)\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Dataset: MUTAG, Number of classes: 2, Number of features: 7\n",
      "For MUTAG dataset with ratio =0.1:\n",
      "k=0.1, test acc: 0.7368421052631579, time/epoch: 0.0290s\n",
      "For MUTAG dataset with ratio =0.2:\n",
      "k=0.2, test acc: 0.7368421052631579, time/epoch: 0.0281s\n",
      "For MUTAG dataset with ratio =0.3:\n",
      "k=0.3, test acc: 0.7631578947368421, time/epoch: 0.0350s\n",
      "For MUTAG dataset with ratio =0.4:\n",
      "k=0.4, test acc: 0.7368421052631579, time/epoch: 0.0256s\n",
      "For MUTAG dataset with ratio =0.5:\n",
      "k=0.5, test acc: 0.7631578947368421, time/epoch: 0.0293s\n",
      "For MUTAG dataset with ratio =0.6:\n",
      "k=0.6, test acc: 0.7368421052631579, time/epoch: 0.0371s\n",
      "For MUTAG dataset with ratio =0.7:\n",
      "k=0.7, test acc: 0.7105263157894737, time/epoch: 0.0374s\n",
      "For MUTAG dataset with ratio =0.8:\n",
      "k=0.8, test acc: 0.7894736842105263, time/epoch: 0.0324s\n",
      "For MUTAG dataset with ratio =0.9:\n",
      "k=0.9, test acc: 0.7894736842105263, time/epoch: 0.0383s\n",
      "Using device: cpu\n",
      "Dataset: PROTEINS, Number of classes: 2, Number of features: 3\n",
      "For PROTEINS dataset with ratio =0.1:\n",
      "k=0.1, test acc: 0.672645739910314, time/epoch: 0.2582s\n",
      "For PROTEINS dataset with ratio =0.2:\n",
      "k=0.2, test acc: 0.6322869955156951, time/epoch: 0.2548s\n",
      "For PROTEINS dataset with ratio =0.3:\n",
      "k=0.3, test acc: 0.6816143497757847, time/epoch: 0.1989s\n",
      "For PROTEINS dataset with ratio =0.4:\n",
      "k=0.4, test acc: 0.6412556053811659, time/epoch: 0.2869s\n",
      "For PROTEINS dataset with ratio =0.5:\n",
      "k=0.5, test acc: 0.6591928251121076, time/epoch: 0.2926s\n",
      "For PROTEINS dataset with ratio =0.6:\n",
      "k=0.6, test acc: 0.6502242152466368, time/epoch: 0.2974s\n",
      "For PROTEINS dataset with ratio =0.7:\n",
      "k=0.7, test acc: 0.695067264573991, time/epoch: 0.3103s\n",
      "For PROTEINS dataset with ratio =0.8:\n",
      "k=0.8, test acc: 0.6591928251121076, time/epoch: 0.2143s\n",
      "For PROTEINS dataset with ratio =0.9:\n",
      "k=0.9, test acc: 0.6591928251121076, time/epoch: 0.2953s\n",
      "Using device: cpu\n",
      "Dataset: ENZYMES, Number of classes: 6, Number of features: 3\n",
      "For ENZYMES dataset with ratio =0.1:\n",
      "k=0.1, test acc: 0.2916666666666667, time/epoch: 0.0711s\n",
      "For ENZYMES dataset with ratio =0.2:\n",
      "k=0.2, test acc: 0.23333333333333334, time/epoch: 0.0730s\n",
      "For ENZYMES dataset with ratio =0.3:\n",
      "k=0.3, test acc: 0.2833333333333333, time/epoch: 0.0761s\n",
      "For ENZYMES dataset with ratio =0.4:\n",
      "k=0.4, test acc: 0.275, time/epoch: 0.1337s\n",
      "For ENZYMES dataset with ratio =0.5:\n",
      "k=0.5, test acc: 0.2916666666666667, time/epoch: 0.1377s\n",
      "For ENZYMES dataset with ratio =0.6:\n",
      "k=0.6, test acc: 0.2833333333333333, time/epoch: 0.1401s\n",
      "For ENZYMES dataset with ratio =0.7:\n",
      "k=0.7, test acc: 0.25833333333333336, time/epoch: 0.1425s\n",
      "For ENZYMES dataset with ratio =0.8:\n",
      "k=0.8, test acc: 0.3, time/epoch: 0.1438s\n",
      "For ENZYMES dataset with ratio =0.9:\n",
      "k=0.9, test acc: 0.2916666666666667, time/epoch: 0.0889s\n",
      "      k   dataset  s/epoch memory_MB  accuracy\n",
      "0   0.1     MUTAG   0.0290       N/A    0.7368\n",
      "1   0.2     MUTAG   0.0281       N/A    0.7368\n",
      "2   0.3     MUTAG   0.0350       N/A    0.7632\n",
      "3   0.4     MUTAG   0.0256       N/A    0.7368\n",
      "4   0.5     MUTAG   0.0293       N/A    0.7632\n",
      "5   0.6     MUTAG   0.0371       N/A    0.7368\n",
      "6   0.7     MUTAG   0.0374       N/A    0.7105\n",
      "7   0.8     MUTAG   0.0324       N/A    0.7895\n",
      "8   0.9     MUTAG   0.0383       N/A    0.7895\n",
      "9   0.1  PROTEINS   0.2582       N/A    0.6726\n",
      "10  0.2  PROTEINS   0.2548       N/A    0.6323\n",
      "11  0.3  PROTEINS   0.1989       N/A    0.6816\n",
      "12  0.4  PROTEINS   0.2869       N/A    0.6413\n",
      "13  0.5  PROTEINS   0.2926       N/A    0.6592\n",
      "14  0.6  PROTEINS   0.2974       N/A    0.6502\n",
      "15  0.7  PROTEINS   0.3103       N/A    0.6951\n",
      "16  0.8  PROTEINS   0.2143       N/A    0.6592\n",
      "17  0.9  PROTEINS   0.2953       N/A    0.6592\n",
      "18  0.1   ENZYMES   0.0711       N/A    0.2917\n",
      "19  0.2   ENZYMES   0.0730       N/A    0.2333\n",
      "20  0.3   ENZYMES   0.0761       N/A    0.2833\n",
      "21  0.4   ENZYMES   0.1337       N/A    0.2750\n",
      "22  0.5   ENZYMES   0.1377       N/A    0.2917\n",
      "23  0.6   ENZYMES   0.1401       N/A    0.2833\n",
      "24  0.7   ENZYMES   0.1425       N/A    0.2583\n",
      "25  0.8   ENZYMES   0.1438       N/A    0.3000\n",
      "26  0.9   ENZYMES   0.0889       N/A    0.2917\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for name in ['MUTAG', 'PROTEINS', 'ENZYMES']:\n",
    "    dataset = TUDataset(root='data', name=name)  # Load the graph classification dataset\n",
    "    dataset = dataset.shuffle()  # Shuffle the dataset\n",
    "\n",
    "    # Split the dataset into training and testing subsets\n",
    "    split = int(0.8 * len(dataset))\n",
    "    train_ds, test_ds = dataset[:split], dataset[split:]\n",
    "    train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(test_ds, batch_size=32)\n",
    "\n",
    "    in_feats = dataset.num_features\n",
    "    hidden_dim = 64\n",
    "    num_classes = dataset.num_classes\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    print(f\"Dataset: {name}, Number of classes: {num_classes}, Number of features: {in_feats}\")\n",
    "\n",
    "    # Grid search over k values\n",
    "    for r in [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]:  # Define the range of k values to test\n",
    "        print(f\"For {name} dataset with ratio ={r}:\")\n",
    "        model = GNNSAG(in_feats, hidden_dim, num_classes, ratio = r).to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "        start_time = time.time()\n",
    "        for epoch in range(1, 101):\n",
    "            loss = train(model, train_loader, optimizer, device)\n",
    "        total_time = time.time() - start_time\n",
    "        time_per_epoch = total_time / 100\n",
    "\n",
    "        acc = test(model, test_loader, device)\n",
    "\n",
    "        if device.type == 'cuda':\n",
    "            peak_mem = torch.cuda.max_memory_allocated(device) / (1024 ** 2)  # Convert to MB\n",
    "        else:\n",
    "            peak_mem = None\n",
    "\n",
    "        print(f\"k={r}, test acc: {acc}, time/epoch: {time_per_epoch:.4f}s\")\n",
    "        # torch.save(model.state_dict(), f\"trained_models/{name}_dmon_k{k}_model.pth\")\n",
    "        results.append({\n",
    "            'k': r,\n",
    "            'dataset': name,\n",
    "            's/epoch': round(time_per_epoch, 4),\n",
    "            'memory_MB': round(peak_mem, 2) if peak_mem is not None else 'N/A',\n",
    "            'accuracy': round(acc, 4)\n",
    "        })\n",
    "\n",
    "        # Cleanup to avoid CUDA OOM across variants\n",
    "        del model\n",
    "        gc.collect()\n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "# Convert to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "# results_df.to_csv(f'gnn_pooling_benchmark_k_grid_search.csv', index=False)\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "For ENZYMES dataset:\n",
      "dmon test acc: 0.2583, time/epoch: 0.1003s\n",
      "ecpool test acc: 0.3167, time/epoch: 0.3749s\n",
      "set2set test acc: 0.4667, time/epoch: 0.2170s\n",
      "topk test acc: 0.2583, time/epoch: 0.0773s\n",
      "sag test acc: 0.3000, time/epoch: 0.0964s\n",
      "diff test acc: 0.4833, time/epoch: 0.1028s\n",
      "      pool  dataset  s/epoch memory_MB  accuracy\n",
      "0     dmon  ENZYMES   0.1003       N/A    0.2583\n",
      "1   ecpool  ENZYMES   0.3749       N/A    0.3167\n",
      "2  set2set  ENZYMES   0.2170       N/A    0.4667\n",
      "3     topk  ENZYMES   0.0773       N/A    0.2583\n",
      "4      sag  ENZYMES   0.0964       N/A    0.3000\n",
      "5     diff  ENZYMES   0.1028       N/A    0.4833\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for name in ['ENZYMES']:\n",
    "    \n",
    "    dataset = TUDataset(root='data', name=name)    # Load the graph classification dataset into a PyTorch Geometric TUDataset object\n",
    "    dataset = dataset.shuffle()  \n",
    "    split = int(0.8 * len(dataset))\n",
    "    train_ds, test_ds = dataset[:split], dataset[split:] \n",
    "    train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "    test_loader  = DataLoader(test_ds,  batch_size=32)\n",
    "    in_feats    = dataset.num_features   # Number of node features per graph\n",
    "    hidden_dim  = 64                     # Hidden dimension size for internal GCN layers\n",
    "    num_classes = dataset.num_classes    # Number of target classes for graph-level classification\n",
    "    device      = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # Compute device\n",
    "    print(f\"Using device: {device}\")\n",
    "    # print(f\"Dataset: {name}, Number of classes: {num_classes}, Number of features: {in_feats}\")\n",
    "    variants = {\n",
    "        'dmon':          GNNDMoN(in_feats, hidden_dim, num_classes, k=11, dropout=0.2),\n",
    "        'ecpool':        GNNECPool(in_feats, hidden_dim, num_classes, ratio=0.1),\n",
    "        # 'mincut':        GNNMinCut(in_feats, hidden_dim, num_classes, k=10, temp = 1.0),\n",
    "        'set2set':     GNNSet2Set(in_feats, hidden_dim, num_classes, 3),\n",
    "        'topk':        GNNTopK(in_feats, hidden_dim, num_classes, 0.4),\n",
    "        'sag':         GNNSAG(in_feats, hidden_dim, num_classes, ratio=0.8),\n",
    "        'diff':        GNNDiffPool(in_feats, assign_dim=hidden_dim, k=11, num_classes=num_classes)\n",
    "    }\n",
    "\n",
    "    print(f\"For {name} dataset:\")\n",
    "    for pool, model in variants.items():\n",
    "        model = model.to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "        start_time = time.time()\n",
    "        for epoch in range(1, 101):\n",
    "            loss = train(model, train_loader, optimizer, device)\n",
    "        total_time = time.time() - start_time\n",
    "        time_per_epoch = total_time / 100\n",
    "\n",
    "        acc = test(model, test_loader, device)\n",
    "\n",
    "        if device.type == 'cuda':\n",
    "            peak_mem = torch.cuda.max_memory_allocated(device) / (1024 ** 2)  # Convert to MB\n",
    "        else:\n",
    "            peak_mem = None  # Could use psutil here for CPU if needed\n",
    "\n",
    "        print(f\"{pool} test acc: {acc:.4f}, time/epoch: {time_per_epoch:.4f}s\")\n",
    "        torch.save(model.state_dict(), f\"optimals/{name}_{pool}_model.pth\")\n",
    "        results.append({\n",
    "            'pool': pool,\n",
    "            'dataset': name,\n",
    "            's/epoch': round(time_per_epoch, 4),\n",
    "            'memory_MB': round(peak_mem, 2) if peak_mem is not None else 'N/A',\n",
    "            'accuracy': round(acc, 4)\n",
    "        })\n",
    "\n",
    "        # Cleanup to avoid CUDA OOM across variants\n",
    "        del model\n",
    "        gc.collect()\n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "# Convert to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(f'gnn_pooling_benchmark_opt.csv', index=False)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/utkarsh/miniconda3/envs/aiml/lib/python3.7/site-packages/torch_geometric/deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "For MUTAG dataset:\n",
      "dmon test acc: 0.9211, time/epoch: 0.0380s\n",
      "ecpool test acc: 0.8684, time/epoch: 0.0538s\n",
      "set2set test acc: 0.8421, time/epoch: 0.0497s\n",
      "topk test acc: 0.8684, time/epoch: 0.0178s\n",
      "sag test acc: 0.8684, time/epoch: 0.0307s\n",
      "diff test acc: 0.8947, time/epoch: 0.0386s\n",
      "      pool dataset  s/epoch memory_MB  accuracy\n",
      "0     dmon   MUTAG   0.0380       N/A    0.9211\n",
      "1   ecpool   MUTAG   0.0538       N/A    0.8684\n",
      "2  set2set   MUTAG   0.0497       N/A    0.8421\n",
      "3     topk   MUTAG   0.0178       N/A    0.8684\n",
      "4      sag   MUTAG   0.0307       N/A    0.8684\n",
      "5     diff   MUTAG   0.0386       N/A    0.8947\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for name in ['MUTAG']:\n",
    "    \n",
    "    dataset = TUDataset(root='data', name=name)    # Load the graph classification dataset into a PyTorch Geometric TUDataset object\n",
    "    dataset = dataset.shuffle()  \n",
    "    split = int(0.8 * len(dataset))\n",
    "    train_ds, test_ds = dataset[:split], dataset[split:] \n",
    "    train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "    test_loader  = DataLoader(test_ds,  batch_size=32)\n",
    "    in_feats    = dataset.num_features   # Number of node features per graph\n",
    "    hidden_dim  = 64                     # Hidden dimension size for internal GCN layers\n",
    "    num_classes = dataset.num_classes    # Number of target classes for graph-level classification\n",
    "    device      = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # Compute device\n",
    "    print(f\"Using device: {device}\")\n",
    "    # print(f\"Dataset: {name}, Number of classes: {num_classes}, Number of features: {in_feats}\")\n",
    "    variants = {\n",
    "        'dmon':          GNNDMoN(in_feats, hidden_dim, num_classes, k=16, dropout=0.2),\n",
    "        'ecpool':        GNNECPool(in_feats, hidden_dim, num_classes, ratio=0.8),\n",
    "        # 'mincut':        GNNMinCut(in_feats, hidden_dim, num_classes, k=10, temp = 1.0),\n",
    "        'set2set':     GNNSet2Set(in_feats, hidden_dim, num_classes, 3),\n",
    "        'topk':        GNNTopK(in_feats, hidden_dim, num_classes, 0.3),\n",
    "        'sag':         GNNSAG(in_feats, hidden_dim, num_classes, ratio=0.9),\n",
    "        'diff':        GNNDiffPool(in_feats, assign_dim=hidden_dim, k=19, num_classes=num_classes)\n",
    "    }\n",
    "\n",
    "    print(f\"For {name} dataset:\")\n",
    "    for pool, model in variants.items():\n",
    "        model = model.to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "        start_time = time.time()\n",
    "        for epoch in range(1, 101):\n",
    "            loss = train(model, train_loader, optimizer, device)\n",
    "        total_time = time.time() - start_time\n",
    "        time_per_epoch = total_time / 100\n",
    "\n",
    "        acc = test(model, test_loader, device)\n",
    "\n",
    "        if device.type == 'cuda':\n",
    "            peak_mem = torch.cuda.max_memory_allocated(device) / (1024 ** 2)  # Convert to MB\n",
    "        else:\n",
    "            peak_mem = None  # Could use psutil here for CPU if needed\n",
    "\n",
    "        print(f\"{pool} test acc: {acc:.4f}, time/epoch: {time_per_epoch:.4f}s\")\n",
    "        torch.save(model.state_dict(), f\"optimals/{name}_{pool}_model.pth\")\n",
    "        results.append({\n",
    "            'pool': pool,\n",
    "            'dataset': name,\n",
    "            's/epoch': round(time_per_epoch, 4),\n",
    "            'memory_MB': round(peak_mem, 2) if peak_mem is not None else 'N/A',\n",
    "            'accuracy': round(acc, 4)\n",
    "        })\n",
    "\n",
    "        # Cleanup to avoid CUDA OOM across variants\n",
    "        del model\n",
    "        gc.collect()\n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "# Convert to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(f'gnn_pooling_benchmark_opt.csv', index=False)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs768",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
