{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/utkarsh/miniconda3/envs/aiml/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from model import GNNGlobal, GNNTopK, GNNSAG, GNNDiffPool, GNNGlobalAttention, GNNSet2Set, GNNDMoN, GNNECPool, GNNMinCut\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.data import DataLoader\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import pandas as pd\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index, data.batch)  # Forward pass\n",
    "        loss = F.cross_entropy(out, data.y)   # Compute cross-entropy loss between predictions and true labels\n",
    "        loss.backward()     # Backpropagation\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "    return total_loss / len(loader.dataset)  # Return average loss per graph             # Return average loss per graph\n",
    "\n",
    "\"\"\"Evaluate model accuracy on the given DataLoader.\"\"\"\n",
    "@torch.no_grad()   # Disables autograd tracking\n",
    "def test(model, loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        pred = model(data.x, data.edge_index, data.batch).argmax(dim=1)   # Predict class by selecting highest logit\n",
    "        correct += (pred == data.y).sum().item()\n",
    "    \n",
    "    return correct / len(loader.dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Dataset: MUTAG, Number of classes: 2, Number of features: 7\n",
      "For MUTAG dataset:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/utkarsh/miniconda3/envs/aiml/lib/python3.7/site-packages/torch_geometric/deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n",
      "/home/utkarsh/miniconda3/envs/aiml/lib/python3.7/site-packages/torch_geometric/deprecation.py:22: UserWarning: 'nn.glob.GlobalAttention' is deprecated, use 'nn.aggr.AttentionalAggregation' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (561) to match target batch_size (32).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4762/1495281012.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m101\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0mtotal_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mtime_per_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_time\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_4762/673977398.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, loader, optimizer, device)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# Compute cross-entropy loss between predictions and true labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;31m# Backpropagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/aiml/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3012\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3013\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3014\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3016\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected input batch_size (561) to match target batch_size (32)."
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for name in ['MUTAG','PROTEINS','ENZYMES']:\n",
    "    \n",
    "    dataset = TUDataset(root='data', name=name)    # Load the graph classification dataset into a PyTorch Geometric TUDataset object\n",
    "    dataset = dataset.shuffle()    # Shuffle the dataset to avoid any potential ordering bias when splitting\n",
    "\n",
    "    # Compute the index at which to split the dataset:\n",
    "    # 80% for training, 20% for testing\n",
    "    split = int(0.8 * len(dataset))\n",
    "    train_ds, test_ds = dataset[:split], dataset[split:]  # Split the shuffled dataset into train and test datasets\n",
    "\n",
    "    # Wrap the training subset in a DataLoader to yield batches of 32 graphs\n",
    "    train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "\n",
    "    # Wrap the test subset in a DataLoader to yield batches of 32 graphs\n",
    "    # during evaluation; shuffling is disabled to preserve consistency\n",
    "    test_loader  = DataLoader(test_ds,  batch_size=32)\n",
    "    \n",
    "    # -----------------------------------------------------------------------------\n",
    "    # 3. Hyperparameters & Device Configuration\n",
    "    # -----------------------------------------------------------------------------\n",
    "\n",
    "    in_feats    = dataset.num_features   # Number of node features per graph\n",
    "    hidden_dim  = 64                     # Hidden dimension size for internal GCN layers\n",
    "    num_classes = dataset.num_classes    # Number of target classes for graph-level classification\n",
    "    device      = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # Compute device\n",
    "    print(f\"Using device: {device}\")\n",
    "    print(f\"Dataset: {name}, Number of classes: {num_classes}, Number of features: {in_feats}\")\n",
    "   # Return accuracy over all graphs\n",
    "\n",
    "    # -----------------------------------------------------------------------------\n",
    "    # 5. Model Instantiation & Training Loop\n",
    "    # -----------------------------------------------------------------------------\n",
    "\n",
    "    # Define different GNN variants to compare\n",
    "    variants = {\n",
    "        'no-pool':     GNNGlobal(in_feats, hidden_dim, num_classes, pool='none'),\n",
    "        'global-max':  GNNGlobal(in_feats, hidden_dim, num_classes, pool='max'),\n",
    "        'dmon':          GNNDMoN(in_feats, hidden_dim, num_classes, k=10, dropout=0.2),\n",
    "        'ecpool':        GNNECPool(in_feats, hidden_dim, num_classes, ratio=0.5),\n",
    "        'mincut':        GNNMinCut(in_feats, hidden_dim, num_classes, k=10, temp = 1.0),\n",
    "        'global-att':  GNNGlobalAttention(in_feats, hidden_dim, num_classes),\n",
    "        'set2set':     GNNSet2Set(in_feats, hidden_dim, num_classes),\n",
    "        'global-mean': GNNGlobal(in_feats, hidden_dim, num_classes, pool='mean'),\n",
    "        'global-sum':  GNNGlobal(in_feats, hidden_dim, num_classes, pool='sum'),\n",
    "        'topk':        GNNTopK(in_feats, hidden_dim, num_classes),\n",
    "        'sag':         GNNSAG(in_feats, hidden_dim, num_classes, ratio=0.5),\n",
    "        'diff':        GNNDiffPool(in_feats, assign_dim=hidden_dim, k=30, num_classes=num_classes)\n",
    "    }\n",
    "\n",
    "    print(f\"For {name} dataset:\")\n",
    "    for pool, model in variants.items():\n",
    "        model = model.to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "        start_time = time.time()\n",
    "        for epoch in range(1, 101):\n",
    "            loss = train(model, train_loader, optimizer, device)\n",
    "        total_time = time.time() - start_time\n",
    "        time_per_epoch = total_time / 100\n",
    "\n",
    "        acc = test(model, test_loader, device)\n",
    "\n",
    "        if device.type == 'cuda':\n",
    "            peak_mem = torch.cuda.max_memory_allocated(device) / (1024 ** 2)  # Convert to MB\n",
    "        else:\n",
    "            peak_mem = None  # Could use psutil here for CPU if needed\n",
    "\n",
    "        print(f\"{pool} test acc: {acc:.4f}, time/epoch: {time_per_epoch:.4f}s, memory: {peak_mem:.2f} MB\")\n",
    "        torch.save(model.state_dict(), f\"trained_models/{name}_{pool}_model.pth\")\n",
    "        results.append({\n",
    "            'pool': pool,\n",
    "            'dataset': name,\n",
    "            's/epoch': round(time_per_epoch, 4),\n",
    "            'memory_MB': round(peak_mem, 2) if peak_mem is not None else 'N/A',\n",
    "            'accuracy': round(acc, 4)\n",
    "        })\n",
    "\n",
    "        # Cleanup to avoid CUDA OOM across variants\n",
    "        del model\n",
    "        gc.collect()\n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "# Convert to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(f'gnn_pooling_benchmark.csv', index=False)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Dataset: MUTAG, Number of classes: 2, Number of features: 7\n",
      "For MUTAG dataset with ratio =0.1:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/utkarsh/miniconda3/envs/aiml/lib/python3.7/site-packages/torch_geometric/deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=0.1, test acc: 0.8157894736842105, time/epoch: 0.0591s\n",
      "For MUTAG dataset with ratio =0.2:\n",
      "k=0.2, test acc: 0.8157894736842105, time/epoch: 0.0523s\n",
      "For MUTAG dataset with ratio =0.3:\n",
      "k=0.3, test acc: 0.8421052631578947, time/epoch: 0.0512s\n",
      "For MUTAG dataset with ratio =0.4:\n",
      "k=0.4, test acc: 0.8421052631578947, time/epoch: 0.0569s\n",
      "For MUTAG dataset with ratio =0.5:\n",
      "k=0.5, test acc: 0.868421052631579, time/epoch: 0.0485s\n",
      "For MUTAG dataset with ratio =0.6:\n",
      "k=0.6, test acc: 0.7894736842105263, time/epoch: 0.0524s\n",
      "For MUTAG dataset with ratio =0.7:\n",
      "k=0.7, test acc: 0.8421052631578947, time/epoch: 0.0514s\n",
      "For MUTAG dataset with ratio =0.8:\n",
      "k=0.8, test acc: 0.868421052631579, time/epoch: 0.0510s\n",
      "For MUTAG dataset with ratio =0.9:\n",
      "k=0.9, test acc: 0.8157894736842105, time/epoch: 0.0599s\n",
      "Using device: cpu\n",
      "Dataset: PROTEINS, Number of classes: 2, Number of features: 3\n",
      "For PROTEINS dataset with ratio =0.1:\n",
      "k=0.1, test acc: 0.7354260089686099, time/epoch: 0.7300s\n",
      "For PROTEINS dataset with ratio =0.2:\n",
      "k=0.2, test acc: 0.7085201793721974, time/epoch: 0.7104s\n",
      "For PROTEINS dataset with ratio =0.3:\n",
      "k=0.3, test acc: 0.7130044843049327, time/epoch: 0.6958s\n",
      "For PROTEINS dataset with ratio =0.4:\n",
      "k=0.4, test acc: 0.695067264573991, time/epoch: 0.6920s\n",
      "For PROTEINS dataset with ratio =0.5:\n",
      "k=0.5, test acc: 0.7174887892376681, time/epoch: 0.6849s\n",
      "For PROTEINS dataset with ratio =0.6:\n",
      "k=0.6, test acc: 0.7309417040358744, time/epoch: 0.6926s\n",
      "For PROTEINS dataset with ratio =0.7:\n",
      "k=0.7, test acc: 0.6905829596412556, time/epoch: 0.7001s\n",
      "For PROTEINS dataset with ratio =0.8:\n",
      "k=0.8, test acc: 0.7040358744394619, time/epoch: 0.6934s\n",
      "For PROTEINS dataset with ratio =0.9:\n",
      "k=0.9, test acc: 0.695067264573991, time/epoch: 0.6933s\n",
      "Using device: cpu\n",
      "Dataset: ENZYMES, Number of classes: 6, Number of features: 3\n",
      "For ENZYMES dataset with ratio =0.1:\n",
      "k=0.1, test acc: 0.2916666666666667, time/epoch: 0.3335s\n",
      "For ENZYMES dataset with ratio =0.2:\n",
      "k=0.2, test acc: 0.26666666666666666, time/epoch: 0.3359s\n",
      "For ENZYMES dataset with ratio =0.3:\n",
      "k=0.3, test acc: 0.25833333333333336, time/epoch: 0.3313s\n",
      "For ENZYMES dataset with ratio =0.4:\n",
      "k=0.4, test acc: 0.25, time/epoch: 0.3326s\n",
      "For ENZYMES dataset with ratio =0.5:\n",
      "k=0.5, test acc: 0.2833333333333333, time/epoch: 0.3297s\n",
      "For ENZYMES dataset with ratio =0.6:\n",
      "k=0.6, test acc: 0.24166666666666667, time/epoch: 0.3281s\n",
      "For ENZYMES dataset with ratio =0.7:\n",
      "k=0.7, test acc: 0.20833333333333334, time/epoch: 0.3332s\n",
      "For ENZYMES dataset with ratio =0.8:\n",
      "k=0.8, test acc: 0.26666666666666666, time/epoch: 0.3352s\n",
      "For ENZYMES dataset with ratio =0.9:\n",
      "k=0.9, test acc: 0.25833333333333336, time/epoch: 0.3376s\n",
      "      k   dataset  s/epoch memory_MB  accuracy\n",
      "0   0.1     MUTAG   0.0591       N/A    0.8158\n",
      "1   0.2     MUTAG   0.0523       N/A    0.8158\n",
      "2   0.3     MUTAG   0.0512       N/A    0.8421\n",
      "3   0.4     MUTAG   0.0569       N/A    0.8421\n",
      "4   0.5     MUTAG   0.0485       N/A    0.8684\n",
      "5   0.6     MUTAG   0.0524       N/A    0.7895\n",
      "6   0.7     MUTAG   0.0514       N/A    0.8421\n",
      "7   0.8     MUTAG   0.0510       N/A    0.8684\n",
      "8   0.9     MUTAG   0.0599       N/A    0.8158\n",
      "9   0.1  PROTEINS   0.7300       N/A    0.7354\n",
      "10  0.2  PROTEINS   0.7104       N/A    0.7085\n",
      "11  0.3  PROTEINS   0.6958       N/A    0.7130\n",
      "12  0.4  PROTEINS   0.6920       N/A    0.6951\n",
      "13  0.5  PROTEINS   0.6849       N/A    0.7175\n",
      "14  0.6  PROTEINS   0.6926       N/A    0.7309\n",
      "15  0.7  PROTEINS   0.7001       N/A    0.6906\n",
      "16  0.8  PROTEINS   0.6934       N/A    0.7040\n",
      "17  0.9  PROTEINS   0.6933       N/A    0.6951\n",
      "18  0.1   ENZYMES   0.3335       N/A    0.2917\n",
      "19  0.2   ENZYMES   0.3359       N/A    0.2667\n",
      "20  0.3   ENZYMES   0.3313       N/A    0.2583\n",
      "21  0.4   ENZYMES   0.3326       N/A    0.2500\n",
      "22  0.5   ENZYMES   0.3297       N/A    0.2833\n",
      "23  0.6   ENZYMES   0.3281       N/A    0.2417\n",
      "24  0.7   ENZYMES   0.3332       N/A    0.2083\n",
      "25  0.8   ENZYMES   0.3352       N/A    0.2667\n",
      "26  0.9   ENZYMES   0.3376       N/A    0.2583\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for name in ['MUTAG', 'PROTEINS', 'ENZYMES']:\n",
    "    dataset = TUDataset(root='data', name=name)  # Load the graph classification dataset\n",
    "    dataset = dataset.shuffle()  # Shuffle the dataset\n",
    "\n",
    "    # Split the dataset into training and testing subsets\n",
    "    split = int(0.8 * len(dataset))\n",
    "    train_ds, test_ds = dataset[:split], dataset[split:]\n",
    "    train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(test_ds, batch_size=32)\n",
    "\n",
    "    in_feats = dataset.num_features\n",
    "    hidden_dim = 64\n",
    "    num_classes = dataset.num_classes\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    print(f\"Dataset: {name}, Number of classes: {num_classes}, Number of features: {in_feats}\")\n",
    "\n",
    "    # Grid search over k values\n",
    "    for r in [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]:  # Define the range of k values to test\n",
    "        print(f\"For {name} dataset with ratio ={r}:\")\n",
    "        model = GNNECPool(in_feats, hidden_dim, num_classes, ratio = r).to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "        start_time = time.time()\n",
    "        for epoch in range(1, 101):\n",
    "            loss = train(model, train_loader, optimizer, device)\n",
    "        total_time = time.time() - start_time\n",
    "        time_per_epoch = total_time / 100\n",
    "\n",
    "        acc = test(model, test_loader, device)\n",
    "\n",
    "        if device.type == 'cuda':\n",
    "            peak_mem = torch.cuda.max_memory_allocated(device) / (1024 ** 2)  # Convert to MB\n",
    "        else:\n",
    "            peak_mem = None\n",
    "\n",
    "        print(f\"k={r}, test acc: {acc}, time/epoch: {time_per_epoch:.4f}s\")\n",
    "        # torch.save(model.state_dict(), f\"trained_models/{name}_dmon_k{k}_model.pth\")\n",
    "        results.append({\n",
    "            'k': r,\n",
    "            'dataset': name,\n",
    "            's/epoch': round(time_per_epoch, 4),\n",
    "            'memory_MB': round(peak_mem, 2) if peak_mem is not None else 'N/A',\n",
    "            'accuracy': round(acc, 4)\n",
    "        })\n",
    "\n",
    "        # Cleanup to avoid CUDA OOM across variants\n",
    "        del model\n",
    "        gc.collect()\n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "# Convert to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "# results_df.to_csv(f'gnn_pooling_benchmark_k_grid_search.csv', index=False)\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Dataset: MUTAG, Number of classes: 2, Number of features: 7\n",
      "For MUTAG dataset with ratio =0.1:\n",
      "k=0.1, test acc: 0.7368421052631579, time/epoch: 0.0290s\n",
      "For MUTAG dataset with ratio =0.2:\n",
      "k=0.2, test acc: 0.7368421052631579, time/epoch: 0.0281s\n",
      "For MUTAG dataset with ratio =0.3:\n",
      "k=0.3, test acc: 0.7631578947368421, time/epoch: 0.0350s\n",
      "For MUTAG dataset with ratio =0.4:\n",
      "k=0.4, test acc: 0.7368421052631579, time/epoch: 0.0256s\n",
      "For MUTAG dataset with ratio =0.5:\n",
      "k=0.5, test acc: 0.7631578947368421, time/epoch: 0.0293s\n",
      "For MUTAG dataset with ratio =0.6:\n",
      "k=0.6, test acc: 0.7368421052631579, time/epoch: 0.0371s\n",
      "For MUTAG dataset with ratio =0.7:\n",
      "k=0.7, test acc: 0.7105263157894737, time/epoch: 0.0374s\n",
      "For MUTAG dataset with ratio =0.8:\n",
      "k=0.8, test acc: 0.7894736842105263, time/epoch: 0.0324s\n",
      "For MUTAG dataset with ratio =0.9:\n",
      "k=0.9, test acc: 0.7894736842105263, time/epoch: 0.0383s\n",
      "Using device: cpu\n",
      "Dataset: PROTEINS, Number of classes: 2, Number of features: 3\n",
      "For PROTEINS dataset with ratio =0.1:\n",
      "k=0.1, test acc: 0.672645739910314, time/epoch: 0.2582s\n",
      "For PROTEINS dataset with ratio =0.2:\n",
      "k=0.2, test acc: 0.6322869955156951, time/epoch: 0.2548s\n",
      "For PROTEINS dataset with ratio =0.3:\n",
      "k=0.3, test acc: 0.6816143497757847, time/epoch: 0.1989s\n",
      "For PROTEINS dataset with ratio =0.4:\n",
      "k=0.4, test acc: 0.6412556053811659, time/epoch: 0.2869s\n",
      "For PROTEINS dataset with ratio =0.5:\n",
      "k=0.5, test acc: 0.6591928251121076, time/epoch: 0.2926s\n",
      "For PROTEINS dataset with ratio =0.6:\n",
      "k=0.6, test acc: 0.6502242152466368, time/epoch: 0.2974s\n",
      "For PROTEINS dataset with ratio =0.7:\n",
      "k=0.7, test acc: 0.695067264573991, time/epoch: 0.3103s\n",
      "For PROTEINS dataset with ratio =0.8:\n",
      "k=0.8, test acc: 0.6591928251121076, time/epoch: 0.2143s\n",
      "For PROTEINS dataset with ratio =0.9:\n",
      "k=0.9, test acc: 0.6591928251121076, time/epoch: 0.2953s\n",
      "Using device: cpu\n",
      "Dataset: ENZYMES, Number of classes: 6, Number of features: 3\n",
      "For ENZYMES dataset with ratio =0.1:\n",
      "k=0.1, test acc: 0.2916666666666667, time/epoch: 0.0711s\n",
      "For ENZYMES dataset with ratio =0.2:\n",
      "k=0.2, test acc: 0.23333333333333334, time/epoch: 0.0730s\n",
      "For ENZYMES dataset with ratio =0.3:\n",
      "k=0.3, test acc: 0.2833333333333333, time/epoch: 0.0761s\n",
      "For ENZYMES dataset with ratio =0.4:\n",
      "k=0.4, test acc: 0.275, time/epoch: 0.1337s\n",
      "For ENZYMES dataset with ratio =0.5:\n",
      "k=0.5, test acc: 0.2916666666666667, time/epoch: 0.1377s\n",
      "For ENZYMES dataset with ratio =0.6:\n",
      "k=0.6, test acc: 0.2833333333333333, time/epoch: 0.1401s\n",
      "For ENZYMES dataset with ratio =0.7:\n",
      "k=0.7, test acc: 0.25833333333333336, time/epoch: 0.1425s\n",
      "For ENZYMES dataset with ratio =0.8:\n",
      "k=0.8, test acc: 0.3, time/epoch: 0.1438s\n",
      "For ENZYMES dataset with ratio =0.9:\n",
      "k=0.9, test acc: 0.2916666666666667, time/epoch: 0.0889s\n",
      "      k   dataset  s/epoch memory_MB  accuracy\n",
      "0   0.1     MUTAG   0.0290       N/A    0.7368\n",
      "1   0.2     MUTAG   0.0281       N/A    0.7368\n",
      "2   0.3     MUTAG   0.0350       N/A    0.7632\n",
      "3   0.4     MUTAG   0.0256       N/A    0.7368\n",
      "4   0.5     MUTAG   0.0293       N/A    0.7632\n",
      "5   0.6     MUTAG   0.0371       N/A    0.7368\n",
      "6   0.7     MUTAG   0.0374       N/A    0.7105\n",
      "7   0.8     MUTAG   0.0324       N/A    0.7895\n",
      "8   0.9     MUTAG   0.0383       N/A    0.7895\n",
      "9   0.1  PROTEINS   0.2582       N/A    0.6726\n",
      "10  0.2  PROTEINS   0.2548       N/A    0.6323\n",
      "11  0.3  PROTEINS   0.1989       N/A    0.6816\n",
      "12  0.4  PROTEINS   0.2869       N/A    0.6413\n",
      "13  0.5  PROTEINS   0.2926       N/A    0.6592\n",
      "14  0.6  PROTEINS   0.2974       N/A    0.6502\n",
      "15  0.7  PROTEINS   0.3103       N/A    0.6951\n",
      "16  0.8  PROTEINS   0.2143       N/A    0.6592\n",
      "17  0.9  PROTEINS   0.2953       N/A    0.6592\n",
      "18  0.1   ENZYMES   0.0711       N/A    0.2917\n",
      "19  0.2   ENZYMES   0.0730       N/A    0.2333\n",
      "20  0.3   ENZYMES   0.0761       N/A    0.2833\n",
      "21  0.4   ENZYMES   0.1337       N/A    0.2750\n",
      "22  0.5   ENZYMES   0.1377       N/A    0.2917\n",
      "23  0.6   ENZYMES   0.1401       N/A    0.2833\n",
      "24  0.7   ENZYMES   0.1425       N/A    0.2583\n",
      "25  0.8   ENZYMES   0.1438       N/A    0.3000\n",
      "26  0.9   ENZYMES   0.0889       N/A    0.2917\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for name in ['MUTAG', 'PROTEINS', 'ENZYMES']:\n",
    "    dataset = TUDataset(root='data', name=name)  # Load the graph classification dataset\n",
    "    dataset = dataset.shuffle()  # Shuffle the dataset\n",
    "\n",
    "    # Split the dataset into training and testing subsets\n",
    "    split = int(0.8 * len(dataset))\n",
    "    train_ds, test_ds = dataset[:split], dataset[split:]\n",
    "    train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(test_ds, batch_size=32)\n",
    "\n",
    "    in_feats = dataset.num_features\n",
    "    hidden_dim = 64\n",
    "    num_classes = dataset.num_classes\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    print(f\"Dataset: {name}, Number of classes: {num_classes}, Number of features: {in_feats}\")\n",
    "\n",
    "    # Grid search over k values\n",
    "    for r in [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]:  # Define the range of k values to test\n",
    "        print(f\"For {name} dataset with ratio ={r}:\")\n",
    "        model = GNNSAG(in_feats, hidden_dim, num_classes, ratio = r).to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "        start_time = time.time()\n",
    "        for epoch in range(1, 101):\n",
    "            loss = train(model, train_loader, optimizer, device)\n",
    "        total_time = time.time() - start_time\n",
    "        time_per_epoch = total_time / 100\n",
    "\n",
    "        acc = test(model, test_loader, device)\n",
    "\n",
    "        if device.type == 'cuda':\n",
    "            peak_mem = torch.cuda.max_memory_allocated(device) / (1024 ** 2)  # Convert to MB\n",
    "        else:\n",
    "            peak_mem = None\n",
    "\n",
    "        print(f\"k={r}, test acc: {acc}, time/epoch: {time_per_epoch:.4f}s\")\n",
    "        # torch.save(model.state_dict(), f\"trained_models/{name}_dmon_k{k}_model.pth\")\n",
    "        results.append({\n",
    "            'k': r,\n",
    "            'dataset': name,\n",
    "            's/epoch': round(time_per_epoch, 4),\n",
    "            'memory_MB': round(peak_mem, 2) if peak_mem is not None else 'N/A',\n",
    "            'accuracy': round(acc, 4)\n",
    "        })\n",
    "\n",
    "        # Cleanup to avoid CUDA OOM across variants\n",
    "        del model\n",
    "        gc.collect()\n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "# Convert to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "# results_df.to_csv(f'gnn_pooling_benchmark_k_grid_search.csv', index=False)\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Dataset: MUTAG, Number of classes: 2, Number of features: 7\n",
      "For MUTAG dataset with k=1:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() got multiple values for argument 'k'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4762/4230913144.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m21\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Define the range of k values to test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"For {name} dataset with k={k}:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGNNDiffPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_feats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got multiple values for argument 'k'"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for name in ['MUTAG', 'PROTEINS', 'ENZYMES']:\n",
    "    dataset = TUDataset(root='data', name=name)  # Load the graph classification dataset\n",
    "    dataset = dataset.shuffle()  # Shuffle the dataset\n",
    "\n",
    "    # Split the dataset into training and testing subsets\n",
    "    split = int(0.8 * len(dataset))\n",
    "    train_ds, test_ds = dataset[:split], dataset[split:]\n",
    "    train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(test_ds, batch_size=32)\n",
    "\n",
    "    in_feats = dataset.num_features\n",
    "    hidden_dim = 64\n",
    "    num_classes = dataset.num_classes\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    print(f\"Dataset: {name}, Number of classes: {num_classes}, Number of features: {in_feats}\")\n",
    "\n",
    "    # Grid search over k values\n",
    "    for k in list(range(1,21)):  # Define the range of k values to test\n",
    "        print(f\"For {name} dataset with k={k}:\")\n",
    "        model = GNNDiffPool(in_feats, hidden_dim, num_classes, k=k, dropout=0.2).to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "        start_time = time.time()\n",
    "        for epoch in range(1, 101):\n",
    "            loss = train(model, train_loader, optimizer, device)\n",
    "        total_time = time.time() - start_time\n",
    "        time_per_epoch = total_time / 100\n",
    "\n",
    "        acc = test(model, test_loader, device)\n",
    "\n",
    "        if device.type == 'cuda':\n",
    "            peak_mem = torch.cuda.max_memory_allocated(device) / (1024 ** 2)  # Convert to MB\n",
    "        else:\n",
    "            peak_mem = None\n",
    "\n",
    "        print(f\"k={k}, test acc: {acc}, time/epoch: {time_per_epoch:.4f}s\")\n",
    "        # torch.save(model.state_dict(), f\"trained_models/{name}_dmon_k{k}_model.pth\")\n",
    "        results.append({\n",
    "            'k': k,\n",
    "            'dataset': name,\n",
    "            's/epoch': round(time_per_epoch, 4),\n",
    "            'memory_MB': round(peak_mem, 2) if peak_mem is not None else 'N/A',\n",
    "            'accuracy': round(acc, 4)\n",
    "        })\n",
    "\n",
    "        # Cleanup to avoid CUDA OOM across variants\n",
    "        del model\n",
    "        gc.collect()\n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "# Convert to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "# results_df.to_csv(f'gnn_pooling_benchmark_k_grid_search.csv', index=False)\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/utkarsh/miniconda3/envs/aiml/lib/python3.7/site-packages/torch_geometric/deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "For PROTEINS dataset:\n",
      "dmon test acc: 0.7982, time/epoch: 0.2099s\n",
      "ecpool test acc: 0.7758, time/epoch: 0.7659s\n",
      "set2set test acc: 0.7713, time/epoch: 0.7103s\n",
      "topk test acc: 0.7085, time/epoch: 0.1718s\n",
      "sag test acc: 0.7040, time/epoch: 0.2145s\n",
      "diff test acc: 0.8386, time/epoch: 0.3548s\n",
      "Using device: cpu\n",
      "For ENZYMES dataset:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4762/1245263939.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m101\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mtotal_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mtime_per_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_time\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_4762/673977398.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, loader, optimizer, device)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# Compute cross-entropy loss between predictions and true labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;31m# Backpropagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_graphs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtotal_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Return average loss per graph             # Return average loss per graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/aiml/lib/python3.7/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/aiml/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/aiml/lib/python3.7/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    169\u001b[0m                  \u001b[0mmaximize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'maximize'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m                  \u001b[0mforeach\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'foreach'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m                  capturable=group['capturable'])\n\u001b[0m\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/aiml/lib/python3.7/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    224\u001b[0m          \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m          \u001b[0mmaximize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaximize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m          capturable=capturable)\n\u001b[0m\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/aiml/lib/python3.7/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m         \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m         \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for name in ['PROTEINS','ENZYMES']:\n",
    "    \n",
    "    dataset = TUDataset(root='data', name=name)    # Load the graph classification dataset into a PyTorch Geometric TUDataset object\n",
    "    dataset = dataset.shuffle()  \n",
    "    split = int(0.8 * len(dataset))\n",
    "    train_ds, test_ds = dataset[:split], dataset[split:] \n",
    "    train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "    test_loader  = DataLoader(test_ds,  batch_size=32)\n",
    "    in_feats    = dataset.num_features   # Number of node features per graph\n",
    "    hidden_dim  = 64                     # Hidden dimension size for internal GCN layers\n",
    "    num_classes = dataset.num_classes    # Number of target classes for graph-level classification\n",
    "    device      = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # Compute device\n",
    "    print(f\"Using device: {device}\")\n",
    "    # print(f\"Dataset: {name}, Number of classes: {num_classes}, Number of features: {in_feats}\")\n",
    "    variants = {\n",
    "        'dmon':          GNNDMoN(in_feats, hidden_dim, num_classes, k=18, dropout=0.2),\n",
    "        'ecpool':        GNNECPool(in_feats, hidden_dim, num_classes, ratio=0.6),\n",
    "        # 'mincut':        GNNMinCut(in_feats, hidden_dim, num_classes, k=10, temp = 1.0),\n",
    "        'set2set':     GNNSet2Set(in_feats, hidden_dim, num_classes, 16),\n",
    "        'topk':        GNNTopK(in_feats, hidden_dim, num_classes, 0.4),\n",
    "        'sag':         GNNSAG(in_feats, hidden_dim, num_classes, ratio=0.9),\n",
    "        'diff':        GNNDiffPool(in_feats, assign_dim=hidden_dim, k=19, num_classes=num_classes)\n",
    "    }\n",
    "\n",
    "    print(f\"For {name} dataset:\")\n",
    "    for pool, model in variants.items():\n",
    "        model = model.to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "        start_time = time.time()\n",
    "        for epoch in range(1, 101):\n",
    "            loss = train(model, train_loader, optimizer, device)\n",
    "        total_time = time.time() - start_time\n",
    "        time_per_epoch = total_time / 100\n",
    "\n",
    "        acc = test(model, test_loader, device)\n",
    "\n",
    "        if device.type == 'cuda':\n",
    "            peak_mem = torch.cuda.max_memory_allocated(device) / (1024 ** 2)  # Convert to MB\n",
    "        else:\n",
    "            peak_mem = None  # Could use psutil here for CPU if needed\n",
    "\n",
    "        print(f\"{pool} test acc: {acc:.4f}, time/epoch: {time_per_epoch:.4f}s\")\n",
    "        torch.save(model.state_dict(), f\"optimals/{name}_{pool}_model.pth\")\n",
    "        results.append({\n",
    "            'pool': pool,\n",
    "            'dataset': name,\n",
    "            's/epoch': round(time_per_epoch, 4),\n",
    "            'memory_MB': round(peak_mem, 2) if peak_mem is not None else 'N/A',\n",
    "            'accuracy': round(acc, 4)\n",
    "        })\n",
    "\n",
    "        # Cleanup to avoid CUDA OOM across variants\n",
    "        del model\n",
    "        gc.collect()\n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "# Convert to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(f'gnn_pooling_benchmark_opt.csv', index=False)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "For ENZYMES dataset:\n",
      "dmon test acc: 0.2583, time/epoch: 0.1003s\n",
      "ecpool test acc: 0.3167, time/epoch: 0.3749s\n",
      "set2set test acc: 0.4667, time/epoch: 0.2170s\n",
      "topk test acc: 0.2583, time/epoch: 0.0773s\n",
      "sag test acc: 0.3000, time/epoch: 0.0964s\n",
      "diff test acc: 0.4833, time/epoch: 0.1028s\n",
      "      pool  dataset  s/epoch memory_MB  accuracy\n",
      "0     dmon  ENZYMES   0.1003       N/A    0.2583\n",
      "1   ecpool  ENZYMES   0.3749       N/A    0.3167\n",
      "2  set2set  ENZYMES   0.2170       N/A    0.4667\n",
      "3     topk  ENZYMES   0.0773       N/A    0.2583\n",
      "4      sag  ENZYMES   0.0964       N/A    0.3000\n",
      "5     diff  ENZYMES   0.1028       N/A    0.4833\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for name in ['ENZYMES']:\n",
    "    \n",
    "    dataset = TUDataset(root='data', name=name)    # Load the graph classification dataset into a PyTorch Geometric TUDataset object\n",
    "    dataset = dataset.shuffle()  \n",
    "    split = int(0.8 * len(dataset))\n",
    "    train_ds, test_ds = dataset[:split], dataset[split:] \n",
    "    train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "    test_loader  = DataLoader(test_ds,  batch_size=32)\n",
    "    in_feats    = dataset.num_features   # Number of node features per graph\n",
    "    hidden_dim  = 64                     # Hidden dimension size for internal GCN layers\n",
    "    num_classes = dataset.num_classes    # Number of target classes for graph-level classification\n",
    "    device      = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # Compute device\n",
    "    print(f\"Using device: {device}\")\n",
    "    # print(f\"Dataset: {name}, Number of classes: {num_classes}, Number of features: {in_feats}\")\n",
    "    variants = {\n",
    "        'dmon':          GNNDMoN(in_feats, hidden_dim, num_classes, k=11, dropout=0.2),\n",
    "        'ecpool':        GNNECPool(in_feats, hidden_dim, num_classes, ratio=0.1),\n",
    "        # 'mincut':        GNNMinCut(in_feats, hidden_dim, num_classes, k=10, temp = 1.0),\n",
    "        'set2set':     GNNSet2Set(in_feats, hidden_dim, num_classes, 3),\n",
    "        'topk':        GNNTopK(in_feats, hidden_dim, num_classes, 0.4),\n",
    "        'sag':         GNNSAG(in_feats, hidden_dim, num_classes, ratio=0.8),\n",
    "        'diff':        GNNDiffPool(in_feats, assign_dim=hidden_dim, k=11, num_classes=num_classes)\n",
    "    }\n",
    "\n",
    "    print(f\"For {name} dataset:\")\n",
    "    for pool, model in variants.items():\n",
    "        model = model.to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "        start_time = time.time()\n",
    "        for epoch in range(1, 101):\n",
    "            loss = train(model, train_loader, optimizer, device)\n",
    "        total_time = time.time() - start_time\n",
    "        time_per_epoch = total_time / 100\n",
    "\n",
    "        acc = test(model, test_loader, device)\n",
    "\n",
    "        if device.type == 'cuda':\n",
    "            peak_mem = torch.cuda.max_memory_allocated(device) / (1024 ** 2)  # Convert to MB\n",
    "        else:\n",
    "            peak_mem = None  # Could use psutil here for CPU if needed\n",
    "\n",
    "        print(f\"{pool} test acc: {acc:.4f}, time/epoch: {time_per_epoch:.4f}s\")\n",
    "        torch.save(model.state_dict(), f\"optimals/{name}_{pool}_model.pth\")\n",
    "        results.append({\n",
    "            'pool': pool,\n",
    "            'dataset': name,\n",
    "            's/epoch': round(time_per_epoch, 4),\n",
    "            'memory_MB': round(peak_mem, 2) if peak_mem is not None else 'N/A',\n",
    "            'accuracy': round(acc, 4)\n",
    "        })\n",
    "\n",
    "        # Cleanup to avoid CUDA OOM across variants\n",
    "        del model\n",
    "        gc.collect()\n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "# Convert to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(f'gnn_pooling_benchmark_opt.csv', index=False)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/utkarsh/miniconda3/envs/aiml/lib/python3.7/site-packages/torch_geometric/deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "For MUTAG dataset:\n",
      "dmon test acc: 0.9211, time/epoch: 0.0380s\n",
      "ecpool test acc: 0.8684, time/epoch: 0.0538s\n",
      "set2set test acc: 0.8421, time/epoch: 0.0497s\n",
      "topk test acc: 0.8684, time/epoch: 0.0178s\n",
      "sag test acc: 0.8684, time/epoch: 0.0307s\n",
      "diff test acc: 0.8947, time/epoch: 0.0386s\n",
      "      pool dataset  s/epoch memory_MB  accuracy\n",
      "0     dmon   MUTAG   0.0380       N/A    0.9211\n",
      "1   ecpool   MUTAG   0.0538       N/A    0.8684\n",
      "2  set2set   MUTAG   0.0497       N/A    0.8421\n",
      "3     topk   MUTAG   0.0178       N/A    0.8684\n",
      "4      sag   MUTAG   0.0307       N/A    0.8684\n",
      "5     diff   MUTAG   0.0386       N/A    0.8947\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for name in ['MUTAG']:\n",
    "    \n",
    "    dataset = TUDataset(root='data', name=name)    # Load the graph classification dataset into a PyTorch Geometric TUDataset object\n",
    "    dataset = dataset.shuffle()  \n",
    "    split = int(0.8 * len(dataset))\n",
    "    train_ds, test_ds = dataset[:split], dataset[split:] \n",
    "    train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "    test_loader  = DataLoader(test_ds,  batch_size=32)\n",
    "    in_feats    = dataset.num_features   # Number of node features per graph\n",
    "    hidden_dim  = 64                     # Hidden dimension size for internal GCN layers\n",
    "    num_classes = dataset.num_classes    # Number of target classes for graph-level classification\n",
    "    device      = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # Compute device\n",
    "    print(f\"Using device: {device}\")\n",
    "    # print(f\"Dataset: {name}, Number of classes: {num_classes}, Number of features: {in_feats}\")\n",
    "    variants = {\n",
    "        'dmon':          GNNDMoN(in_feats, hidden_dim, num_classes, k=16, dropout=0.2),\n",
    "        'ecpool':        GNNECPool(in_feats, hidden_dim, num_classes, ratio=0.8),\n",
    "        # 'mincut':        GNNMinCut(in_feats, hidden_dim, num_classes, k=10, temp = 1.0),\n",
    "        'set2set':     GNNSet2Set(in_feats, hidden_dim, num_classes, 3),\n",
    "        'topk':        GNNTopK(in_feats, hidden_dim, num_classes, 0.3),\n",
    "        'sag':         GNNSAG(in_feats, hidden_dim, num_classes, ratio=0.9),\n",
    "        'diff':        GNNDiffPool(in_feats, assign_dim=hidden_dim, k=19, num_classes=num_classes)\n",
    "    }\n",
    "\n",
    "    print(f\"For {name} dataset:\")\n",
    "    for pool, model in variants.items():\n",
    "        model = model.to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "        start_time = time.time()\n",
    "        for epoch in range(1, 101):\n",
    "            loss = train(model, train_loader, optimizer, device)\n",
    "        total_time = time.time() - start_time\n",
    "        time_per_epoch = total_time / 100\n",
    "\n",
    "        acc = test(model, test_loader, device)\n",
    "\n",
    "        if device.type == 'cuda':\n",
    "            peak_mem = torch.cuda.max_memory_allocated(device) / (1024 ** 2)  # Convert to MB\n",
    "        else:\n",
    "            peak_mem = None  # Could use psutil here for CPU if needed\n",
    "\n",
    "        print(f\"{pool} test acc: {acc:.4f}, time/epoch: {time_per_epoch:.4f}s\")\n",
    "        torch.save(model.state_dict(), f\"optimals/{name}_{pool}_model.pth\")\n",
    "        results.append({\n",
    "            'pool': pool,\n",
    "            'dataset': name,\n",
    "            's/epoch': round(time_per_epoch, 4),\n",
    "            'memory_MB': round(peak_mem, 2) if peak_mem is not None else 'N/A',\n",
    "            'accuracy': round(acc, 4)\n",
    "        })\n",
    "\n",
    "        # Cleanup to avoid CUDA OOM across variants\n",
    "        del model\n",
    "        gc.collect()\n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "# Convert to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(f'gnn_pooling_benchmark_opt.csv', index=False)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
