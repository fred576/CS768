{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import GNNGlobal, GNNTopK, GNNSAG, GNNDiffPool, GNNGlobalAttention, GNNSet2Set, GNNDMoN, GNNECPool, GNNMinCut\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.data import DataLoader\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import pandas as pd\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index, data.batch)  # Forward pass\n",
    "        loss = F.cross_entropy(out, data.y)   # Compute cross-entropy loss between predictions and true labels\n",
    "        loss.backward()     # Backpropagation\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "    return total_loss / len(loader.dataset)  # Return average loss per graph             # Return average loss per graph\n",
    "\n",
    "\"\"\"Evaluate model accuracy on the given DataLoader.\"\"\"\n",
    "@torch.no_grad()   # Disables autograd tracking\n",
    "def test(model, loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        pred = model(data.x, data.edge_index, data.batch).argmax(dim=1)   # Predict class by selecting highest logit\n",
    "        correct += (pred == data.y).sum().item()\n",
    "    \n",
    "    return correct / len(loader.dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/ug22/anileshbansal/.local/lib/python3.9/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Dataset: MUTAG, Number of classes: 2, Number of features: 7\n",
      "For MUTAG dataset:\n",
      "dmon test acc: 0.8158, time/epoch: 0.0554s, memory: 18.66 MB\n",
      "Using device: cuda\n",
      "Dataset: PROTEINS, Number of classes: 2, Number of features: 3\n",
      "For PROTEINS dataset:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2522018/318096183.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m101\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0mtotal_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mtime_per_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_time\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2522018/673977398.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, loader, optimizer, device)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# Compute cross-entropy loss between predictions and true labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;31m# Backpropagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/CS768/project/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, edge_index, batch)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;31m# 2) Convert to dense formats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m         \u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_dense_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m        \u001b[0;31m# [B, N_max, H]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0mA_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_dense_adj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m       \u001b[0;31m# [B, N_max, N_max]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0;31m# 3) DMoN pooling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch_geometric/experimental.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_experimental_mode_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'disable_dynamic_shapes'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mrequired_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrequired_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch_geometric/utils/_to_dense_batch.py\u001b[0m in \u001b[0;36mto_dense_batch\u001b[0;34m(x, batch, fill_value, max_num_nodes, batch_size)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mcum_nodes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m     \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmax_num_nodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilter_nodes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mmax_num_nodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for name in ['MUTAG','PROTEINS','ENZYMES']:\n",
    "    \n",
    "    dataset = TUDataset(root='data', name=name)    # Load the graph classification dataset into a PyTorch Geometric TUDataset object\n",
    "    dataset = dataset.shuffle()    # Shuffle the dataset to avoid any potential ordering bias when splitting\n",
    "\n",
    "    # Compute the index at which to split the dataset:\n",
    "    # 80% for training, 20% for testing\n",
    "    split = int(0.8 * len(dataset))\n",
    "    train_ds, test_ds = dataset[:split], dataset[split:]  # Split the shuffled dataset into train and test datasets\n",
    "\n",
    "    # Wrap the training subset in a DataLoader to yield batches of 32 graphs\n",
    "    train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "\n",
    "    # Wrap the test subset in a DataLoader to yield batches of 32 graphs\n",
    "    # during evaluation; shuffling is disabled to preserve consistency\n",
    "    test_loader  = DataLoader(test_ds,  batch_size=32)\n",
    "    \n",
    "    # -----------------------------------------------------------------------------\n",
    "    # 3. Hyperparameters & Device Configuration\n",
    "    # -----------------------------------------------------------------------------\n",
    "\n",
    "    in_feats    = dataset.num_features   # Number of node features per graph\n",
    "    hidden_dim  = 64                     # Hidden dimension size for internal GCN layers\n",
    "    num_classes = dataset.num_classes    # Number of target classes for graph-level classification\n",
    "    device      = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # Compute device\n",
    "    print(f\"Using device: {device}\")\n",
    "    print(f\"Dataset: {name}, Number of classes: {num_classes}, Number of features: {in_feats}\")\n",
    "   # Return accuracy over all graphs\n",
    "\n",
    "    # -----------------------------------------------------------------------------\n",
    "    # 5. Model Instantiation & Training Loop\n",
    "    # -----------------------------------------------------------------------------\n",
    "\n",
    "    # Define different GNN variants to compare\n",
    "    variants = {\n",
    "        'no-pool':     GNNGlobal(in_feats, hidden_dim, num_classes, pool='none'),\n",
    "        'global-max':  GNNGlobal(in_feats, hidden_dim, num_classes, pool='max'),\n",
    "        'dmon':          GNNDMoN(in_feats, hidden_dim, num_classes, k=10, dropout=0.2),\n",
    "        'ecpool':        GNNECPool(in_feats, hidden_dim, num_classes, ratio=0.5),\n",
    "        'mincut':        GNNMinCut(in_feats, hidden_dim, num_classes, k=10, temp = 1.0),\n",
    "        'global-att':  GNNGlobalAttention(in_feats, hidden_dim, num_classes),\n",
    "        'set2set':     GNNSet2Set(in_feats, hidden_dim, num_classes),\n",
    "        'global-mean': GNNGlobal(in_feats, hidden_dim, num_classes, pool='mean'),\n",
    "        'global-sum':  GNNGlobal(in_feats, hidden_dim, num_classes, pool='sum'),\n",
    "        'topk':        GNNTopK(in_feats, hidden_dim, num_classes),\n",
    "        'sag':         GNNSAG(in_feats, hidden_dim, num_classes, ratio=0.5),\n",
    "        'diff':        GNNDiffPool(in_feats, assign_dim=hidden_dim, k=30, num_classes=num_classes)\n",
    "    }\n",
    "\n",
    "    print(f\"For {name} dataset:\")\n",
    "    for pool, model in variants.items():\n",
    "        model = model.to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "        start_time = time.time()\n",
    "        for epoch in range(1, 101):\n",
    "            loss = train(model, train_loader, optimizer, device)\n",
    "        total_time = time.time() - start_time\n",
    "        time_per_epoch = total_time / 100\n",
    "\n",
    "        acc = test(model, test_loader, device)\n",
    "\n",
    "        if device.type == 'cuda':\n",
    "            peak_mem = torch.cuda.max_memory_allocated(device) / (1024 ** 2)  # Convert to MB\n",
    "        else:\n",
    "            peak_mem = None  # Could use psutil here for CPU if needed\n",
    "\n",
    "        print(f\"{pool} test acc: {acc:.4f}, time/epoch: {time_per_epoch:.4f}s, memory: {peak_mem:.2f} MB\")\n",
    "        torch.save(model.state_dict(), f\"trained_models/{name}_{pool}_model.pth\")\n",
    "        results.append({\n",
    "            'pool': pool,\n",
    "            'dataset': name,\n",
    "            's/epoch': round(time_per_epoch, 4),\n",
    "            'memory_MB': round(peak_mem, 2) if peak_mem is not None else 'N/A',\n",
    "            'accuracy': round(acc, 4)\n",
    "        })\n",
    "\n",
    "        # Cleanup to avoid CUDA OOM across variants\n",
    "        del model\n",
    "        gc.collect()\n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "# Convert to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(f'gnn_pooling_benchmark.csv', index=False)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Dataset: MUTAG, Number of classes: 2, Number of features: 7\n",
      "For MUTAG dataset with ratio =0.1:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/utkarsh/miniconda3/envs/aiml/lib/python3.7/site-packages/torch_geometric/deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=0.1, test acc: 0.8157894736842105, time/epoch: 0.0591s\n",
      "For MUTAG dataset with ratio =0.2:\n",
      "k=0.2, test acc: 0.8157894736842105, time/epoch: 0.0523s\n",
      "For MUTAG dataset with ratio =0.3:\n",
      "k=0.3, test acc: 0.8421052631578947, time/epoch: 0.0512s\n",
      "For MUTAG dataset with ratio =0.4:\n",
      "k=0.4, test acc: 0.8421052631578947, time/epoch: 0.0569s\n",
      "For MUTAG dataset with ratio =0.5:\n",
      "k=0.5, test acc: 0.868421052631579, time/epoch: 0.0485s\n",
      "For MUTAG dataset with ratio =0.6:\n",
      "k=0.6, test acc: 0.7894736842105263, time/epoch: 0.0524s\n",
      "For MUTAG dataset with ratio =0.7:\n",
      "k=0.7, test acc: 0.8421052631578947, time/epoch: 0.0514s\n",
      "For MUTAG dataset with ratio =0.8:\n",
      "k=0.8, test acc: 0.868421052631579, time/epoch: 0.0510s\n",
      "For MUTAG dataset with ratio =0.9:\n",
      "k=0.9, test acc: 0.8157894736842105, time/epoch: 0.0599s\n",
      "Using device: cpu\n",
      "Dataset: PROTEINS, Number of classes: 2, Number of features: 3\n",
      "For PROTEINS dataset with ratio =0.1:\n",
      "k=0.1, test acc: 0.7354260089686099, time/epoch: 0.7300s\n",
      "For PROTEINS dataset with ratio =0.2:\n",
      "k=0.2, test acc: 0.7085201793721974, time/epoch: 0.7104s\n",
      "For PROTEINS dataset with ratio =0.3:\n",
      "k=0.3, test acc: 0.7130044843049327, time/epoch: 0.6958s\n",
      "For PROTEINS dataset with ratio =0.4:\n",
      "k=0.4, test acc: 0.695067264573991, time/epoch: 0.6920s\n",
      "For PROTEINS dataset with ratio =0.5:\n",
      "k=0.5, test acc: 0.7174887892376681, time/epoch: 0.6849s\n",
      "For PROTEINS dataset with ratio =0.6:\n",
      "k=0.6, test acc: 0.7309417040358744, time/epoch: 0.6926s\n",
      "For PROTEINS dataset with ratio =0.7:\n",
      "k=0.7, test acc: 0.6905829596412556, time/epoch: 0.7001s\n",
      "For PROTEINS dataset with ratio =0.8:\n",
      "k=0.8, test acc: 0.7040358744394619, time/epoch: 0.6934s\n",
      "For PROTEINS dataset with ratio =0.9:\n",
      "k=0.9, test acc: 0.695067264573991, time/epoch: 0.6933s\n",
      "Using device: cpu\n",
      "Dataset: ENZYMES, Number of classes: 6, Number of features: 3\n",
      "For ENZYMES dataset with ratio =0.1:\n",
      "k=0.1, test acc: 0.2916666666666667, time/epoch: 0.3335s\n",
      "For ENZYMES dataset with ratio =0.2:\n",
      "k=0.2, test acc: 0.26666666666666666, time/epoch: 0.3359s\n",
      "For ENZYMES dataset with ratio =0.3:\n",
      "k=0.3, test acc: 0.25833333333333336, time/epoch: 0.3313s\n",
      "For ENZYMES dataset with ratio =0.4:\n",
      "k=0.4, test acc: 0.25, time/epoch: 0.3326s\n",
      "For ENZYMES dataset with ratio =0.5:\n",
      "k=0.5, test acc: 0.2833333333333333, time/epoch: 0.3297s\n",
      "For ENZYMES dataset with ratio =0.6:\n",
      "k=0.6, test acc: 0.24166666666666667, time/epoch: 0.3281s\n",
      "For ENZYMES dataset with ratio =0.7:\n",
      "k=0.7, test acc: 0.20833333333333334, time/epoch: 0.3332s\n",
      "For ENZYMES dataset with ratio =0.8:\n",
      "k=0.8, test acc: 0.26666666666666666, time/epoch: 0.3352s\n",
      "For ENZYMES dataset with ratio =0.9:\n",
      "k=0.9, test acc: 0.25833333333333336, time/epoch: 0.3376s\n",
      "      k   dataset  s/epoch memory_MB  accuracy\n",
      "0   0.1     MUTAG   0.0591       N/A    0.8158\n",
      "1   0.2     MUTAG   0.0523       N/A    0.8158\n",
      "2   0.3     MUTAG   0.0512       N/A    0.8421\n",
      "3   0.4     MUTAG   0.0569       N/A    0.8421\n",
      "4   0.5     MUTAG   0.0485       N/A    0.8684\n",
      "5   0.6     MUTAG   0.0524       N/A    0.7895\n",
      "6   0.7     MUTAG   0.0514       N/A    0.8421\n",
      "7   0.8     MUTAG   0.0510       N/A    0.8684\n",
      "8   0.9     MUTAG   0.0599       N/A    0.8158\n",
      "9   0.1  PROTEINS   0.7300       N/A    0.7354\n",
      "10  0.2  PROTEINS   0.7104       N/A    0.7085\n",
      "11  0.3  PROTEINS   0.6958       N/A    0.7130\n",
      "12  0.4  PROTEINS   0.6920       N/A    0.6951\n",
      "13  0.5  PROTEINS   0.6849       N/A    0.7175\n",
      "14  0.6  PROTEINS   0.6926       N/A    0.7309\n",
      "15  0.7  PROTEINS   0.7001       N/A    0.6906\n",
      "16  0.8  PROTEINS   0.6934       N/A    0.7040\n",
      "17  0.9  PROTEINS   0.6933       N/A    0.6951\n",
      "18  0.1   ENZYMES   0.3335       N/A    0.2917\n",
      "19  0.2   ENZYMES   0.3359       N/A    0.2667\n",
      "20  0.3   ENZYMES   0.3313       N/A    0.2583\n",
      "21  0.4   ENZYMES   0.3326       N/A    0.2500\n",
      "22  0.5   ENZYMES   0.3297       N/A    0.2833\n",
      "23  0.6   ENZYMES   0.3281       N/A    0.2417\n",
      "24  0.7   ENZYMES   0.3332       N/A    0.2083\n",
      "25  0.8   ENZYMES   0.3352       N/A    0.2667\n",
      "26  0.9   ENZYMES   0.3376       N/A    0.2583\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for name in ['MUTAG', 'PROTEINS', 'ENZYMES']:\n",
    "    dataset = TUDataset(root='data', name=name)  # Load the graph classification dataset\n",
    "    dataset = dataset.shuffle()  # Shuffle the dataset\n",
    "\n",
    "    # Split the dataset into training and testing subsets\n",
    "    split = int(0.8 * len(dataset))\n",
    "    train_ds, test_ds = dataset[:split], dataset[split:]\n",
    "    train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(test_ds, batch_size=32)\n",
    "\n",
    "    in_feats = dataset.num_features\n",
    "    hidden_dim = 64\n",
    "    num_classes = dataset.num_classes\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    print(f\"Dataset: {name}, Number of classes: {num_classes}, Number of features: {in_feats}\")\n",
    "\n",
    "    # Grid search over k values\n",
    "    for r in [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]:  # Define the range of k values to test\n",
    "        print(f\"For {name} dataset with ratio ={r}:\")\n",
    "        model = GNNECPool(in_feats, hidden_dim, num_classes, ratio = r).to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "        start_time = time.time()\n",
    "        for epoch in range(1, 101):\n",
    "            loss = train(model, train_loader, optimizer, device)\n",
    "        total_time = time.time() - start_time\n",
    "        time_per_epoch = total_time / 100\n",
    "\n",
    "        acc = test(model, test_loader, device)\n",
    "\n",
    "        if device.type == 'cuda':\n",
    "            peak_mem = torch.cuda.max_memory_allocated(device) / (1024 ** 2)  # Convert to MB\n",
    "        else:\n",
    "            peak_mem = None\n",
    "\n",
    "        print(f\"k={r}, test acc: {acc}, time/epoch: {time_per_epoch:.4f}s\")\n",
    "        # torch.save(model.state_dict(), f\"trained_models/{name}_dmon_k{k}_model.pth\")\n",
    "        results.append({\n",
    "            'k': r,\n",
    "            'dataset': name,\n",
    "            's/epoch': round(time_per_epoch, 4),\n",
    "            'memory_MB': round(peak_mem, 2) if peak_mem is not None else 'N/A',\n",
    "            'accuracy': round(acc, 4)\n",
    "        })\n",
    "\n",
    "        # Cleanup to avoid CUDA OOM across variants\n",
    "        del model\n",
    "        gc.collect()\n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "# Convert to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "# results_df.to_csv(f'gnn_pooling_benchmark_k_grid_search.csv', index=False)\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/utkarsh/miniconda3/envs/aiml/lib/python3.7/site-packages/torch_geometric/deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Dataset: MUTAG, Number of classes: 2, Number of features: 7\n",
      "For MUTAG dataset with ratio =0.1:\n",
      "k=0.1, test acc: 0.6052631578947368, time/epoch: 0.0352s\n",
      "For MUTAG dataset with ratio =0.2:\n",
      "k=0.2, test acc: 0.9210526315789473, time/epoch: 0.0330s\n",
      "For MUTAG dataset with ratio =0.3:\n",
      "k=0.3, test acc: 0.8421052631578947, time/epoch: 0.0298s\n",
      "For MUTAG dataset with ratio =0.4:\n",
      "k=0.4, test acc: 0.868421052631579, time/epoch: 0.0380s\n",
      "For MUTAG dataset with ratio =0.5:\n",
      "k=0.5, test acc: 0.7105263157894737, time/epoch: 0.0350s\n",
      "For MUTAG dataset with ratio =0.6:\n",
      "k=0.6, test acc: 0.7894736842105263, time/epoch: 0.0318s\n",
      "For MUTAG dataset with ratio =0.7:\n",
      "k=0.7, test acc: 0.868421052631579, time/epoch: 0.0345s\n",
      "For MUTAG dataset with ratio =0.8:\n",
      "k=0.8, test acc: 0.868421052631579, time/epoch: 0.0281s\n",
      "For MUTAG dataset with ratio =0.9:\n",
      "k=0.9, test acc: 0.9210526315789473, time/epoch: 0.0288s\n",
      "Using device: cpu\n",
      "Dataset: PROTEINS, Number of classes: 2, Number of features: 3\n",
      "For PROTEINS dataset with ratio =0.1:\n",
      "k=0.1, test acc: 0.7309417040358744, time/epoch: 0.2420s\n",
      "For PROTEINS dataset with ratio =0.2:\n",
      "k=0.2, test acc: 0.6278026905829597, time/epoch: 0.1642s\n",
      "For PROTEINS dataset with ratio =0.3:\n",
      "k=0.3, test acc: 0.6591928251121076, time/epoch: 0.1358s\n",
      "For PROTEINS dataset with ratio =0.4:\n",
      "k=0.4, test acc: 0.6681614349775785, time/epoch: 0.2530s\n",
      "For PROTEINS dataset with ratio =0.5:\n",
      "k=0.5, test acc: 0.695067264573991, time/epoch: 0.2602s\n",
      "For PROTEINS dataset with ratio =0.6:\n",
      "k=0.6, test acc: 0.6591928251121076, time/epoch: 0.2529s\n",
      "For PROTEINS dataset with ratio =0.7:\n",
      "k=0.7, test acc: 0.6547085201793722, time/epoch: 0.2413s\n",
      "For PROTEINS dataset with ratio =0.8:\n",
      "k=0.8, test acc: 0.6591928251121076, time/epoch: 0.2218s\n",
      "For PROTEINS dataset with ratio =0.9:\n",
      "k=0.9, test acc: 0.6681614349775785, time/epoch: 0.2854s\n",
      "Using device: cpu\n",
      "Dataset: ENZYMES, Number of classes: 6, Number of features: 3\n",
      "For ENZYMES dataset with ratio =0.1:\n",
      "k=0.1, test acc: 0.2833333333333333, time/epoch: 0.0708s\n",
      "For ENZYMES dataset with ratio =0.2:\n",
      "k=0.2, test acc: 0.25833333333333336, time/epoch: 0.0738s\n",
      "For ENZYMES dataset with ratio =0.3:\n",
      "k=0.3, test acc: 0.21666666666666667, time/epoch: 0.0756s\n",
      "For ENZYMES dataset with ratio =0.4:\n",
      "k=0.4, test acc: 0.3, time/epoch: 0.1296s\n",
      "For ENZYMES dataset with ratio =0.5:\n",
      "k=0.5, test acc: 0.26666666666666666, time/epoch: 0.1332s\n",
      "For ENZYMES dataset with ratio =0.6:\n",
      "k=0.6, test acc: 0.3, time/epoch: 0.0814s\n",
      "For ENZYMES dataset with ratio =0.7:\n",
      "k=0.7, test acc: 0.275, time/epoch: 0.0845s\n",
      "For ENZYMES dataset with ratio =0.8:\n",
      "k=0.8, test acc: 0.2833333333333333, time/epoch: 0.0864s\n",
      "For ENZYMES dataset with ratio =0.9:\n",
      "k=0.9, test acc: 0.275, time/epoch: 0.0884s\n",
      "      k   dataset  s/epoch memory_MB  accuracy\n",
      "0   0.1     MUTAG   0.0352       N/A    0.6053\n",
      "1   0.2     MUTAG   0.0330       N/A    0.9211\n",
      "2   0.3     MUTAG   0.0298       N/A    0.8421\n",
      "3   0.4     MUTAG   0.0380       N/A    0.8684\n",
      "4   0.5     MUTAG   0.0350       N/A    0.7105\n",
      "5   0.6     MUTAG   0.0318       N/A    0.7895\n",
      "6   0.7     MUTAG   0.0345       N/A    0.8684\n",
      "7   0.8     MUTAG   0.0281       N/A    0.8684\n",
      "8   0.9     MUTAG   0.0288       N/A    0.9211\n",
      "9   0.1  PROTEINS   0.2420       N/A    0.7309\n",
      "10  0.2  PROTEINS   0.1642       N/A    0.6278\n",
      "11  0.3  PROTEINS   0.1358       N/A    0.6592\n",
      "12  0.4  PROTEINS   0.2530       N/A    0.6682\n",
      "13  0.5  PROTEINS   0.2602       N/A    0.6951\n",
      "14  0.6  PROTEINS   0.2529       N/A    0.6592\n",
      "15  0.7  PROTEINS   0.2413       N/A    0.6547\n",
      "16  0.8  PROTEINS   0.2218       N/A    0.6592\n",
      "17  0.9  PROTEINS   0.2854       N/A    0.6682\n",
      "18  0.1   ENZYMES   0.0708       N/A    0.2833\n",
      "19  0.2   ENZYMES   0.0738       N/A    0.2583\n",
      "20  0.3   ENZYMES   0.0756       N/A    0.2167\n",
      "21  0.4   ENZYMES   0.1296       N/A    0.3000\n",
      "22  0.5   ENZYMES   0.1332       N/A    0.2667\n",
      "23  0.6   ENZYMES   0.0814       N/A    0.3000\n",
      "24  0.7   ENZYMES   0.0845       N/A    0.2750\n",
      "25  0.8   ENZYMES   0.0864       N/A    0.2833\n",
      "26  0.9   ENZYMES   0.0884       N/A    0.2750\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for name in ['MUTAG', 'PROTEINS', 'ENZYMES']:\n",
    "    dataset = TUDataset(root='data', name=name)  # Load the graph classification dataset\n",
    "    dataset = dataset.shuffle()  # Shuffle the dataset\n",
    "\n",
    "    # Split the dataset into training and testing subsets\n",
    "    split = int(0.8 * len(dataset))\n",
    "    train_ds, test_ds = dataset[:split], dataset[split:]\n",
    "    train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(test_ds, batch_size=32)\n",
    "\n",
    "    in_feats = dataset.num_features\n",
    "    hidden_dim = 64\n",
    "    num_classes = dataset.num_classes\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    print(f\"Dataset: {name}, Number of classes: {num_classes}, Number of features: {in_feats}\")\n",
    "\n",
    "    # Grid search over k values\n",
    "    for r in [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]:  # Define the range of k values to test\n",
    "        print(f\"For {name} dataset with ratio ={r}:\")\n",
    "        model = GNNSAG(in_feats, hidden_dim, num_classes, ratio = r).to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "        start_time = time.time()\n",
    "        for epoch in range(1, 101):\n",
    "            loss = train(model, train_loader, optimizer, device)\n",
    "        total_time = time.time() - start_time\n",
    "        time_per_epoch = total_time / 100\n",
    "\n",
    "        acc = test(model, test_loader, device)\n",
    "\n",
    "        if device.type == 'cuda':\n",
    "            peak_mem = torch.cuda.max_memory_allocated(device) / (1024 ** 2)  # Convert to MB\n",
    "        else:\n",
    "            peak_mem = None\n",
    "\n",
    "        print(f\"k={r}, test acc: {acc}, time/epoch: {time_per_epoch:.4f}s\")\n",
    "        # torch.save(model.state_dict(), f\"trained_models/{name}_dmon_k{k}_model.pth\")\n",
    "        results.append({\n",
    "            'k': r,\n",
    "            'dataset': name,\n",
    "            's/epoch': round(time_per_epoch, 4),\n",
    "            'memory_MB': round(peak_mem, 2) if peak_mem is not None else 'N/A',\n",
    "            'accuracy': round(acc, 4)\n",
    "        })\n",
    "\n",
    "        # Cleanup to avoid CUDA OOM across variants\n",
    "        del model\n",
    "        gc.collect()\n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "# Convert to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "# results_df.to_csv(f'gnn_pooling_benchmark_k_grid_search.csv', index=False)\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Dataset: MUTAG, Number of classes: 2, Number of features: 7\n",
      "For MUTAG dataset with k=1:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/utkarsh/miniconda3/envs/aiml/lib/python3.7/site-packages/torch_geometric/deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() got multiple values for argument 'k'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_354185/4230913144.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m21\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Define the range of k values to test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"For {name} dataset with k={k}:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGNNDiffPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_feats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got multiple values for argument 'k'"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for name in ['MUTAG', 'PROTEINS', 'ENZYMES']:\n",
    "    dataset = TUDataset(root='data', name=name)  # Load the graph classification dataset\n",
    "    dataset = dataset.shuffle()  # Shuffle the dataset\n",
    "\n",
    "    # Split the dataset into training and testing subsets\n",
    "    split = int(0.8 * len(dataset))\n",
    "    train_ds, test_ds = dataset[:split], dataset[split:]\n",
    "    train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(test_ds, batch_size=32)\n",
    "\n",
    "    in_feats = dataset.num_features\n",
    "    hidden_dim = 64\n",
    "    num_classes = dataset.num_classes\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    print(f\"Dataset: {name}, Number of classes: {num_classes}, Number of features: {in_feats}\")\n",
    "\n",
    "    # Grid search over k values\n",
    "    for k in list(range(1,21)):  # Define the range of k values to test\n",
    "        print(f\"For {name} dataset with k={k}:\")\n",
    "        model = GNNDiffPool(in_feats, hidden_dim, num_classes, k=k, dropout=0.2).to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "        start_time = time.time()\n",
    "        for epoch in range(1, 101):\n",
    "            loss = train(model, train_loader, optimizer, device)\n",
    "        total_time = time.time() - start_time\n",
    "        time_per_epoch = total_time / 100\n",
    "\n",
    "        acc = test(model, test_loader, device)\n",
    "\n",
    "        if device.type == 'cuda':\n",
    "            peak_mem = torch.cuda.max_memory_allocated(device) / (1024 ** 2)  # Convert to MB\n",
    "        else:\n",
    "            peak_mem = None\n",
    "\n",
    "        print(f\"k={k}, test acc: {acc}, time/epoch: {time_per_epoch:.4f}s\")\n",
    "        # torch.save(model.state_dict(), f\"trained_models/{name}_dmon_k{k}_model.pth\")\n",
    "        results.append({\n",
    "            'k': k,\n",
    "            'dataset': name,\n",
    "            's/epoch': round(time_per_epoch, 4),\n",
    "            'memory_MB': round(peak_mem, 2) if peak_mem is not None else 'N/A',\n",
    "            'accuracy': round(acc, 4)\n",
    "        })\n",
    "\n",
    "        # Cleanup to avoid CUDA OOM across variants\n",
    "        del model\n",
    "        gc.collect()\n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "# Convert to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "# results_df.to_csv(f'gnn_pooling_benchmark_k_grid_search.csv', index=False)\n",
    "print(results_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
