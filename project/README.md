# Graph Neural Network Pooling Benchmarks and Robustness Analysis

This repository contains code for benchmarking various Graph Neural Network (GNN) pooling methods on graph classification tasks and analyzing their robustness to perturbations.

## Repository Structure
```
.
├── data/
├── grid_search_results/
├── imgoutputs/
├── optimal_models/
├── analyze.ipynb
├── grid_search.ipynb
├── main.py
├── model.py
├── robust.py
└── README.md
```
## How to Run the Code

1.  **Install Dependencies:**
    * `torch`
    * `torch_geometric`
    <!-- * `pandas`
    * `numpy`
    * `tqdm`
    * `matplotlib` (for visualization in `robust.py` and `analyze.ipynb`)
    * `seaborn` (for visualization in `analyze.ipynb`)
    * It is recommended to create a virtual environment to manage dependencies.  For example, using conda:
        ```bash
        conda create -n gnn python=3.7  # or your preferred python version
        conda activate gnn
        conda install pytorch -c pytorch
        conda install pyg -c pyg
        conda install pandas numpy tqdm
        conda install matplotlib seaborn  # If you need the visualization -->
2.  **Training and Evaluation:**
    * To train and evaluate the models, run `main.py`:
        ```bash
        python main.py --dataset MUTAG --out results.csv
        ```
        You can specify the dataset and output filename as command-line arguments.
3.  **Grid Search:**
    * To run the hyperparameter grid search, execute the `grid_search.ipynb` notebook.
4.  **Robustness Analysis:**
    * To perform the robustness analysis, run `robust.py`:
        ```bash
        python robust.py --dataset MUTAG --out robustness_results.csv --runs 10
        ```
        This will evaluate the models' performance under edge removal and feature noise. Adjust `--runs` to control the number of repetitions for each perturbation level.
    * To analyze the results, run the `analyze.ipynb` notebook.  This will generate visualizations from the `robustness_results.csv` file.

## File/Module Implementations

### `model.py`

This module defines the GNN models used in the project. It leverages the `torch_geometric` library.  The following models are implemented:

* **`GNNGlobal`**: A simple 2-layer GCN with global mean/sum/max pooling.
* **`GNNTopK`**: GCN with Top-K pooling, which selects the top nodes based on their projection.
* **`GNNSAG`**:  Self-Attention Graph (SAG) Pooling, where node importance is learned via attention.
* **`GNNDiffPool`**:  Differentiable Pooling (DiffPool), which learns a soft cluster assignment.
* **`GNNGlobalAttention`**: GCN with global attention pooling, where the contribution of each node to the graph representation is learned.
* **`GNNSet2Set`**:  GCN with Set2Set pooling, which iteratively combines node embeddings.
* **`GNNDMoN`**:  Deep Modularity Network (DMoN) Pooling.
* **`GNNECPool`**:  EdgeConditionedPool
* **`GNNMinCut`**:  MinCut pooling, which aims to minimize the cut between clusters.

It also imports and uses components from `torch_geometric` such as:

* `torch_geometric.nn`: GCNConv, various global pooling layers, TopKPooling, SAGPooling, GlobalAttention, Set2Set, EdgePooling
* `torch_geometric.data`: DataLoader, Data, TUDataset
* `torch_geometric.utils`: subgraph, to\_dense\_adj, to\_dense\_batch
* `torch_geometric.nn.dense`: DMoNPooling, dense\_mincut\_pool

### `main.py`

This is the main script for training and evaluating the GNN models defined in `model.py`.

* It loads datasets from the TUDataset collection.
* It initializes and trains the specified GNN model variants.
* It evaluates the trained models on a test set.
* It prints and saves the test accuracy, training time, and memory usage.
* It saves the trained model weights.

The script includes `train()` and `test()` functions to handle the training and evaluation loops.

### `grid_search.ipynb`

This Jupyter Notebook performs a hyperparameter grid search to find optimal configurations for the GNN models.

* It imports the models from `model.py` and uses `torch_geometric` for data loading.
* It defines the `train()` and `test()` functions (identical to `main.py`).
* It iterates over different hyperparameter combinations
* It saves the results of the grid search to a CSV file.

### `analyze.ipynb`

This Jupyter Notebook analyzes the robustness results generated by `robust.py`.

* It uses `pandas` to load the results from a CSV file.
* It uses `matplotlib` and `seaborn` to generate visualizations of the robustness analysis, including line plots and heatmaps.
* It visualizes the impact of different perturbation types and levels on the accuracy of the GNN models.

### `robust.py`

This script evaluates the robustness of the GNN pooling variants to perturbations.

* It defines functions to introduce perturbations to the graph data:
    * `drop_edges()`: Randomly removes edges from the graph.
    * `add_feature_noise()`: Adds Gaussian noise to node features.
* It loads datasets using `torch_geometric.datasets.TUDataset`.
* It iterates over different perturbation levels and runs the training and evaluation multiple times to get statistical significance.
* It calculates and saves the mean and standard deviation of the accuracy for each model and perturbation level.
* It optionally visualizes the results using `matplotlib` and `seaborn`.
* It uses `tqdm` to show progress bars.
* It takes command-line arguments for dataset selection, output filename, and number of runs.
